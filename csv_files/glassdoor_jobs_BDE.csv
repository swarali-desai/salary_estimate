Company Name,Competitors,Founded,Headquarters,Industry,Job Description,Job Title,Location,Rating,Revenue,Salary Estimate,Sector,Size,Type of ownership
"Amazon.com, Inc.
4.3",-1,1994,-1,Internet,"We need TM1 application developers! Within Amazons Corporate Financial Planning & Analysis team (FP&A), we enjoy a unique vantage point into everything happening within Amazon. As part of that, our team designs, builds & maintains an integrated planning & reporting platform that enables hundreds of finance representatives across the globe.
In addition to requiring a high level of technical expertise within Cognos TM1, this role is about understanding the needs of the business, the data behind it, and how to transform this information into technical solutions that allow the business to make swift, informed decisions. Ideal candidates will have expertise in all phases of the software development life cycle in building models that scale over time while balancing accuracy, flexibility and speed.
The data flowing through our platform directly contributes to decision-making by our CFO and all levels of finance leadership. If youre passionate about building tools that enhance productivity, improve financial accuracy, reduce waste, and improve work-life harmony for a large and rapidly growing finance user base, come join us!





Basic Qualifications

- Bachelor's degree in Information Systems, Computer Science, Finance, Accounting, Economics, Mathematics or a related technical discipline.
- 4+ years of Cognos TM1 work experience in developing end-to-end analytics solutions, including system configuration, model building, data security, and reporting.


Preferred Qualifications

- Experience advocating and proliferating best practices in reporting and analysis, including data integrity, test design, analysis, validation, and documentation - Experience in providing risk assessment for new functionality and enhancements, and identify process improvement opportunities to drive innovation - Relevant corporate finance experience exhibiting knowledge of financial planning functions and related processes. - Excellent communication (verbal and written) and interpersonal skills, and an ability to effectively communicate with both business and technical teams. - Quick learner with a positive attitude and professional demeanor, and strong analytical and troubleshooting skills.- Direct experience using Business Intelligence (BI) reporting tools, preferably Cognos v10x (Report Studio, Analysis Studio, Workspace, Framework Manager), or similar technology (OBIEE, Business Objects, Tableau, MicroStrategy, SAP, etc.). - Experience developing scorecards and dashboards. - Experience with dimension management software processes and MDM tools (Oracle DRM) - Advanced knowledge of SQL, Oracle, OLAP, and data warehousing concepts. - Familiarity and development experience with web services technologies is a strong plus (e.g. HTTPS, REST, XML, JSON, etc). - JAVA/PERL/Ruby/Python scripting language experience. - Experience with third-party ETL tools (IBM DataStage or Data Manager, Informatica, etc). - Familiarity with AWS solutions such as EC2, DynamoDB, S3, Redshift, and Aurora.
Amazon is an Equal Opportunity-Affirmative Action Employer Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation",Big Data Engineer,Bengaluru,4.3,$10+ billion (USD),"INR  14,39,361",Information Technology,10000+ Employees,Company - Public
"Tredence Analytics Solutions Private Limited
3.5",-1,2013,-1,Consulting,"Big Data Engineer

Bengaluru, India
JOB DESCRIPTION

Develops data architectures and pipelines by;
Understanding requirements;
Understanding deadlines;
Understanding systems flow, data usage, and work processes;
Working in tandem with the Analytics team and providing inputs as and when required
Investigating problem areas;
Follow Agile and Scrum practices;
Be thought partner to the onsite counterpart in creating the data charter for the client
Should be able to lead client calls and answer design specific queries
Documents and demonstrates solutions by developing documentation in the forms of flowcharts, layouts, diagrams, charts, code comments and clear code;
Provides information by collecting, analyzing, and summarizing development and service issues;
Accomplishes engineering and organization mission by completing related results as needed and on time with high quality;
THE IDEAL CANDIDATE WILL

Thorough understanding of the entire Hadoop Ecosystem
Should have created convoluted data-pipelines
Should have expertise with more than one of the following Hive, Map Reduce, Pig, Oozy, Spark
Should have worked with clusters from different distributors namely MapR, CloudEra, Hortonworks
Should have worked extensively on data modeling and data lake creation
Should be able to estimate, design pipelines based on High data volumes and create base data assets for analytical work to commence
Proficient with Ubuntu/Linux and shell scripting
Should be aware of one Scripting language namely Python/Scala/Java
Expert in performance tuning Hive queries based on storage formats and partitioning
Strong fundamentals in SQL
Experience with Streaming data sources and NoSql databases is a plus
ELIGIBILITY CRITERIA

2 to 6 years of relevant experience
Bachelor’s and/or Master’s degree in computer science or equivalent experience
Strong communication, analytical and problem-solving skills with a high attention to detail
Send your CV to careers@tredence.com",Big Data Engineer,Bengaluru,3.5,$10 to $25 million (USD),"INR  6,80,638",Business Services,501 to 1000 Employees,Company - Private
"Condeco
3.8",-1,2005,-1,Computer Hardware & Software,"Title: Data Engineer

Skills: Data Warehouse, Big Data, Power BI, Hadoop, ETL, Machine Learning, Python, cloud, Data Governance, Mongo DB

Location: DLF Cyber Hub, Gurugram

Company Summary:
We’re the global leader in workspace scheduling technology. We make it easy to find and book space to meet up and work together. We provide workspace scheduling software to over 1,000 of the world’s biggest brands, integrating meeting room and workspace reservation solutions that help remove friction in the workplace and free businesses and their people up to get the most out of their working day.

Responsibilities:
Building the right infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Building data pipelines.
Data warehousing, and building ETL pipelines.
Understand the Business context, objectives and requirements.
Involvement in Requirement Analysis, Design, Development, Unit Testing, System Integration Testing and other facets of testing for example but not limited to Performance Testing.
Convert functional specifications from business requirements into programming instructions for technical development of analytics data pipelines.
Develop industrialized solutions leveraging Agile and DevOps methodologies.
Monitor operating efficiency and optimize solution execution performance.
Work with stakeholders including the product owner, data and design teams to assist with data-related technical issues and support their data infrastructure needs.
Create and maintain optimal data pipeline architecture.
Solve Big Data and Distributed Data Streaming problems using latest technologies.
Perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Manipulate, process and extract value from large disconnected datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
About You:
Experience with Power BI or other BI tools such as Qlikview, Tableau, Spotfire, Cognos is required.
Hadoop, Spark, Kafka, Scala, Python, DB2, MySQL, Oracle, no-SQL databases like MongoDB, Elastic Search.
Preferred Technical and Professional Expertise.
Enterprise data lakes, data analytics, reporting, in-memory data handling, enterprise integration tools, etc.
Experience with data warehousing and building ETL pipelines.
Good understanding of industry best practices for data governance and security.
Possess strong knowledge in designing database models to store structured & unstructured data efficiently and in creating effective data tools for analytics experts.
Comfortable in building effective analytical tools that utilize the data pipeline to provide actionable insights into data synchronization, reporting, operational efficiency and related areas.
Expert in setting up effective pipelines to capture data from multiple sources into the enterprise centric storage.
Good communication skills.
Benefits:
Health insurance fully paid – Spouse, children and Parents
Accident insurance fully paid
Transport allowance
Gratuity fully paid
25 days holiday
7 paid sick days
10 public holidays
Company Information:
Follow us on Twitter | LinkedIn | YouTube
Condeco are proud to be an equal opportunity employer. We are committed to treating all individuals in a fair and equal manner by creating an inclusive and open environment for all employees",Data Engineer,Gurgaon,3.8,$25 to $50 million (USD),"INR  16,37,214",Information Technology,201 to 500 Employees,Company - Private
"Colgate-Palmolive Company
4.0",-1,1806,-1,Consumer Products Manufacturing,"Relocation Assistance Offered Within Country
# 83461 - Mumbai, Maharashtra, India

We are looking for a savvy Data Engineer to join our growing team of analytics experts. Data Engineers will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoy optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

Roles and Responsibility:

Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Assist with data-related technical issues and support their data infrastructure needs.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.

Experience

We are looking for a candidate with minimum 2 years of experience in a Data Engineer role
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Experience supporting and working with cross-functional teams in a dynamic environment.
They should also have experience using the following software/tools:
Experience with relational SQL and NoSQL databases: MongoDB, Neo4j, etc
Experience with cloud services: GCP, AWS, etc
Experience with object-oriented/object function scripting languages: Python, Java, etc.
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with Data Flow, Data Pipeline and workflow management tools: Cloud Composer, Airflow, Luigi, etc.

Qualification & Competencies

Bachelor’s degree required, Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field is preferred
Working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Strong analytic skills related to working with unstructured datasets.
Strong problem solving skills with an emphasis on product development.
Strong experience with test driven development methodologies.
Strong oral & written communication skills with an ability to express complex technical concepts in business terms and business needs in technical specifications
A drive to learn and master new technologies and techniques.

Equal Opportunity Employer
Colgate is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity, sexual orientation, national origin, ethnicity, age, disability, marital status, veteran status (United States positions), or any other characteristic protected by law.

Are you interested in working for Colgate-Palmolive? You can apply online and attach all relevant documents such as a cover letter and resume or CV. Applications received by e-mail are not considered in the selection process. Become part of our team. We look forward to your application.

Colgate-Palmolive is a leading global consumer products company, tightly focused on Oral Care, Personal Care, Home Care and Pet Nutrition. Colgate sells its products in over 200 countries and territories around the world under such internationally recognized brand names as Colgate, Palmolive, elmex, Tom’s of Maine, Sorriso, Speed Stick, Lady Speed Stick, Softsoap, Irish Spring, Protex, Sanex, Elta MD, PCA Skin, Ajax, Axion, Fabuloso, Soupline and Suavitel, as well as Hill’s Science Diet and Hill’s Prescription Diet.

For more information about Colgate’s global business, visit the Company’s web site at http://www.colgatepalmolive.com. To learn more about Colgate Bright Smiles, Bright Futures® oral health education program, please visit http://www.colgatebsbf.com. To learn more about Hill's and the Hill’s Food, Shelter & Love program please visit http://www.hillspet.com. To learn more about Tom’s of Maine please visit http://www.tomsofmaine.com.

Reasonable accommodation during the application process is available for persons with disabilities. Please contact Application_Accommodation@colpal.com with the subject ""Accommodation Request"" should you require accommodation.",Data Engineer II / Sr Data Engineer,Mumbai,4.0,$10+ billion (USD),"INR  7,19,698",Manufacturing,10000+ Employees,Company - Public
"phData
2.9",-1,2014,-1,IT Services,"About phData

We build next-generation strategic platforms helping customers to save money and unlock real business value for our customers and the community. If you are inspired by innovation, hard work, and a passion for data, we want to hear from you.
Our Commitment – you will be working in a fast-moving environment with the brightest and most experienced minds in technology. We're committed to constant learning and innovation.
Our Work – you'll be helping companies answer questions and create products that, until now, were too big, too expensive, and too complex to accomplish.
Our Technology – we focus on building and deploying disruptive big data technologies. If you have experience in or passion for Hadoop and its supporting technologies, we want to hear from you!
In addition to the phenomenal growth and learning opportunity, we offer competitive compensation and excellent perks including base salary, annual bonus, and extensive training - in addition to generous PTO, flexible hours and comprehensive Insurance cover.

Passionate about Big Data, Data Science, and Machine Learning Algorithms? Be part of a team of industry pioneering experts that operate some of the largest analytics and data science infrastructure systems at phData. Our customers apply these technologies on terabytes of data a day on petabyte-scale infrastructure.

Job Information

Technical Responsibilities
Hands-on Engineering of Data Processing applications
Experience with Big Data technologies, assisting customers in building software solutions on-prem or cloud.
Very strong understanding of SQL alongside traditional/conventional data warehousing design patterns.
Hands-on experience in developing Big Data applications using Hadoop technologies such as Spark (Scala or PySpark), Map Reduce (Java), YARN, HDFS, Hive, Impala, Sqoop, Oozie, HBase, Kudu.
Good understanding of UNIX shell scripting.
Develop analytical functionality and complex transformation that will be finally deployed in production data platforms.
Good understanding of Big data design patterns
Hands-on experience troubleshooting, optimizing, and enhancing the big data pipeline and bring improvements.
Experience & good proficiency with at least one programming language (Java, Scala, Python)
Extensive experience with at least one major Hadoop platform (Cloudera, Hortonworks, MapR)
Worked and deployed at least one data engineering application using cloud-native data platform and services (example EMR or Redshift in AWS or Data factory in Azure or BigQuery in GCP)
Strong troubleshooting and performance tuning skills.
Very well versed with continuous integration and deployment procedure considering the bigdata stack on-prem or cloud environment.
Ability to analyze business requirement user stories and translate them into system requirement specifications.
Experience working under the agile delivery methodology.
Behavioral Requirement
Demonstrated ability to work independently
Good communication skills and documentation skills.
Good & Collaborative Team Player.
Good organizational and time management skills
An ability to work to deadlines
A good eye for detail
Must be ready to learn and adapt to new technologies.


Qualifications Requirements
BE/BTech in computer science Or MCA with sound industry experience (2-4 years)
A minimum of 2 years' experience in hands-on development
Experience in Java, Scala or Python (Production level coding)
Experience in a cloud-based environment with PaaS & IaaS
Work iteratively in a team with continuous collaboration",Data Engineer,Bengaluru,2.9,Unknown / Non-Applicable,"INR 403K - INR 2,066K",Information Technology,51 to 200 Employees,Company - Private
"Blue Jeans Network, Inc.
3.0",-1,2009,-1,Enterprise Software & Network Solutions,"Develop maintain and enhance Bluejeans's Big data warehouse using state of the art ETL frameworks and technologies

Description
Develop and extend ETL pipeline for a number data systems used by Engineering, Operations, Sales, Marketing and Finance and product.
Build and integrate reporting system for ad-hoc queries and business dashboards
Help architect our Big Data system, using as much as practical third-party solutions
Work with outside vendors and other staff on integration projects
Define Data Quality and Data Governance processes to help the system scale
Help resolve data issues, troubleshoot system problems and assesses priorities
As needed, assist other staff with reporting, debugging data accuracy issues and other related functions.


Qualifications :
3-6 years of strong experience in developing reporting/ETL solutions using any standard third party tools such as Informatica, Abnitio, Tableau
Experience with distributed data processing technologies such as Spark/Hadoop/EMR
Programming experience in of python or relatedto implement ETL pipelines
Strong SQL, database and SQL performance tuning skills
Strong experience in understanding business requirements and dimensional modelling
Exposure in developing real time pipelines using Kafka/Kinesis or related
Exposure to any third party Reporting tools such as Tableau/Cognos/Business objects preferable
Exposure to machine learning a plus
Verizon recently acquired BlueJeans and plans to integrate BlueJeans employees into Verizon, including its compensation and benefits programs, in due course. This position will be part of that planned integration.",Data Engineer,Bengaluru,3.0,Unknown / Non-Applicable,"INR  25,20,522",Information Technology,201 to 500 Employees,Company - Private
"Deutsche Bank AG
3.5",-1,1870,-1,Banks & Credit Unions,"Job Description:


Job Title: Associate – Big Data Scala

Location: Pune, India

The Engineer designs and develops application code, implements technical solutions, and configures applications in different environments in response to business problems. To meet the requirements of the business, the Engineer actively participates in the design and architecture of the application or its components, investigates and proposes appropriate technologies to be used, promotes re-usability of existing components and contributes to the creation of frameworks. Assists more junior members of the team and controls their work where applicable.

What we’ll offer you

As part of our flexible scheme, here are just some of the benefits that you’ll enjoy
Best in class leave policy
Gender neutral parental leaves
100% reimbursement under child care assistance benefit (gender neutral)
Flexible working arrangements
Sponsorship for Industry relevant certifications and education
Employee Assistance Program for you and your family members
Comprehensive Hospitalization Insurance for you and your dependents
Accident and Term life Insurance
Complementary Health screening for 35 yrs. and above
Your key responsibilities

Essential:
Hadoop Cloudera distribution – Manager, Navigator
Apache Spark – Streaming , structured streaming, and batch processing.
Big data SQL Engines – Hive & Impala
Scala – Core, Concurrency, Akka Actors
Apache Kafka – Publish , subscribe, partitioning, delivery semantics
HDFS – Access APIs, Operations, CLI
Oracle – SQL, PL/SQL

Nice to have:
Python 3 through the Anaconda distribution
Python Analytics libraries: Numpy, Pandas, Koalas, Arrow, Matplotlob etc
Apache Airflow, Tensorflow, Spark MLib, Scikit
Jupyter Notebooks/Hub
Apache Flink, Flume, Sqoop
Caching Tech – EhCache, Apache Ignite
Cassandra, HBase

How we’ll support you
Training and development to help you excel in your career
Flexible working to assist you balance your personal priorities
Coaching and support from experts in your team
A culture of continuous learning to aid progression
A range of flexible benefits that you can tailor to suit your needs
About us and our teams

Please visit our company website for further information:

https://www.db.com/company/company.htm

Our values define the working environment we strive to create – diverse, supportive and welcoming of different views. We embrace a culture reflecting a variety of perspectives, insights and backgrounds to drive innovation. We build talented and diverse teams to drive business results and encourage our people to develop to their full potential. Talk to us about flexible work arrangements and other initiatives we offer.

We promote good working relationships and encourage high standards of conduct and work performance. We welcome applications from talented people from all cultures, countries, races, genders, sexual orientations, disabilities, beliefs and generations and are committed to providing a working environment free from harassment, discrimination and retaliation.

Click here to find out more about our diversity and inclusion policy and initiatives.",Big Data - Scala - Engineer,Pune,3.5,$10+ billion (USD),"INR  17,08,991",Finance,10000+ Employees,Company - Public
"Snowflake Inc.
4.1",-1,2012,-1,Enterprise Software & Network Solutions,"Snowflake started with a clear vision: develop a cloud data platform that is effective, affordable, and accessible to all data users. Snowflake developed an innovative new product with a built-for-the-cloud architecture that combines the power of data warehousing, the flexibility of big data platforms, and the elasticity of the cloud at a fraction of the cost of traditional solutions. We are now a global, world-class organization with offices in more than a dozen countries and serving many more.

We're looking for a strong Data Engineer to build state of the art Data pipelines for snowflake . In this role, you will work closely with many cross-functional teams to build a data pipeline to ingest data into our internal Snowflake environment . This is a strategic, high-impact role that will also help shape the future of Snowflake products and services.

RESPONSIBILITIES:


Architect & build data pipelines for snowflake .
Manage and improve the data integrity and reliability of data services.
Build data frameworks to onboard new data into our snowflake data warehouse
Foster collaboration among engineering teams, IT & other business groups to ensure data access is secure & are audit-able.
Train distributed team members in data pipelines.
WHAT YOU WILL NEED:


5+ years of experience in Data warehousing, data modeling, and SQL .
3 + years of experience in working on public cloud (AWS, Azure or GCP)
3+ years of experience in MPP or Cloud data warehouse solutions like Snowflake, Redshift, BigQuery or Teradata
Experience in ELT based data pipeline build outs is useful..
Experience with sourcing and modeling data from application APIs.
Strong communication and cross functional collaboration skill

PREFFERED QUALIFICATIONS:


B.S or MS in Computer Science or equivalent practical experience.
5 + years experience in building data pipelines using Python/Java & SQL
Experience in ETL tools is nice to have.
Snowflake is growing fast, and we're scaling our team to help enable and accelerate our growth. We are looking for people who share our values, challenge ordinary thinking, and push the pace of innovation while building a future for themselves and Snowflake.

How do you want to make your impact?

Snowflake is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, color, gender identity or expression, marital status, national origin, disability, protected veteran status, race, religion, pregnancy, sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances.",Data Engineer,Pune,4.1,Unknown / Non-Applicable,"INR 2,219K - INR 2,400K",Information Technology,1001 to 5000 Employees,Company - Private
"JPMorgan Chase & Co.
3.9",-1,1799,-1,Investment Banking & Asset Management,"As a member of our Software Engineering Group, we look first and foremost for people who are passionate around solving business problems through innovation and engineering practices. You'll be required to apply your depth of knowledge and expertise to all aspects of the software development lifecycle, as well as partner continuously with your many stakeholders on a daily basis to stay focused on common goals. We embrace a culture of experimentation and constantly strive for improvement and learning. You'll work in a collaborative, trusting, thought-provoking environment-one that encourages diversity of thought and creative solutions that are in the best interests of our customers globally.

We are looking for a Software Engineer with experience in data warehousing and big data (Hadoop-based data platforms) domain. The role will be responsible for development of a data warehouse migration to a next generation enterprise data hub (EDH). The project will include the migration of reports (Cognos), analytical tools (QlikSense, Qlik View) and applications onto the new environment.
Senior developer/mentor to an existing team, designing solutions, best practices, defining standards and innovative solutions
Senior Dev Ops / TDD Journey inc CICD, Scanning, Code, Performance testing and Test coverage
Development using Agile methodologies (Jira / Scrum) delivered on time and in budget
Transforming existing ETL logic into Hadoop Platform in accordance with Architecture diagrams, performing required analysis & create Jiras
Innovate new ways of managing, transforming and validating data
Embed a culture of quality assurance best practices to the team
Data Quality Management - Establish and enforce guidelines to ensure consistency, quality and completeness of data
Learning cutting edge technologies and applications to greenfield projects
Support on legacy jobs which is primarily developed using unix shell scripts and oracle
This role requires a wide variety of strengths and capabilities, including:
Minimum 4+ Experience in a Big Data technology (Hadoop and Spark Architecture, Spark SQL, HIVE, SQOOP, Impala, HBASE, Entitlements etc., )
4+ years of Experience in Data warehouse or Database back ground
Strong Experience with UNIX shell scripting
Experience with Datawarehouses
Experience in Map Reduce
Experienced in writing SQL queries
Communicate complex issues in a crisp and concise fashion to multiple levels of management
Excellent interpersonal skills necessary to work effectively with colleagues at various levels of the organization and across multiple locations
Experience in the below would be advantageous:
Experience in cloud implementation
Experience in implementing complex ETL transformations in Hadoop platform
Experience with Python
Experience performing data analytics on Hadoop-based platforms
Experience in implementing distributed and scalable algorithms (Hadoop, Spark)
Experience in DevOps","Software Engineering - Big Data, Data warehousing - Bangalore",Bengaluru,3.9,$10+ billion (USD),"INR  16,32,068",Finance,10000+ Employees,Company - Public
"Telstra Corporation Limited
3.7",-1,1901,-1,Telecommunications Services,"Employment Type
Permanent
Closing Date
11 Aug 2020 11:59pm
Job Title
Data Engineer
Job Description


Telstra is Australia’s leading telecommunications and technology company, with operations in more than 20 countries, including in India where we’ve launched our new Innovation and Capability Centre (ICC) in Bangalore.

We’re combining innovation, automation and technology to solve the world’s biggest technological challenges in areas such as Internet of Things (IoT), 5G, Artificial Intelligence (AI), Machine Learning, and more. Join us on this exciting journey, and together, we’ll reimagine the future.

Our Software Engineering teams are building a new platform to support Microservice APIs, developed by our teams of developers spread across the globe. We're using industry leading technologies and design principles to encourage best practice application design / development and operation, such as automation and CI/CD.

As a Data Engineer, you will develop, maintain, test and evaluate big data solutions within organisations and would also be involved in the design of big data solutions. You will plan, coordinate, and execute all activities related to the requirements interpretation, design and implementation of data analytics applications.

In this role, your key responsibilities are…
Design and develop data analytics applications.
Review vendor designs and recommend solutions based on industry best practices.
Provide technical governance across data analytics solutions at Telstra.
Assess and improve the efficiency and effectiveness of the data analytics application solutions to ensure user requirements and business objectives are met in a timely and cost-effective manner.
Understand overall business landscape and develops innovative solutions to help improve productivity
Coordinate with technical resource within and outside of Feature team.
Monitor process of software configuration/development/testing to assure quality deliverable.
Ensure standards of QA are being met.
Review deliverables to verify that they meet client and contract expectations; Implement and enforce high standards for quality deliverables.
Analyses performance and capacity issues of the highest complexity.
Assists leadership with development and management of new application capabilities to improve productivity.
Provide training and educate other team members around core capabilities and helps them deliver high quality solutions and deliverables/documentation.
Design/develop user requirements, test and deploy the changes into production.
To be successful in the role, you must have…
Degree level IT qualifications in Software or Systems Engineering.
Minimum 3 years of experience in IT of building and supporting Data Engineering pipeline using Big Data Platform.
Extensive experience using Hortonworks Data Platform.
Experience in Apache Hive, Spark, Zeppelin and Ambari.
Proven expertise of working on Azure Cloud using Databricks, Azure Event hub Flume/Kafka/Spark streaming, Azure Data Factory.
Experience of using Informatica for ETL use cases.
Experience of working in Azure DevOps and ARM.
Experience of using Control-m to schedule data engineering workflows.
Experience in designing solutions for multiple large data warehouses with a good understanding of cluster and parallel architecture as well as high-scale or distributed RDBMS and/or knowledge on NoSQL platforms.
Experience in translating, loading and presenting disparate datasets in multiple formats/sources including JSON, XML etc.
Good knowledge of NoSQL Databases/ HBase/ MongoDB.
Experience in Programming: Java Spring Boot/ Scala/ Python / SQL and Multi-tenant databases.
Strong problem solving and analytical skills.
Demonstrated high level of written and oral communication skills.
Proven high level of initiative, drive and enthusiasm with excellent time management and an ability to work under pressure.
Desirable to have experience on:
Automation using PowerShell
Rabbit MQ
Jenkins
Power BI
REST API
Salesforce
Our people in India will be at the forefront of technological change as they work collaboratively with, and learn from, world-class experts and have access to the latest training programs and insights for their field.

Alongside your work on leading edge projects, working with us means you'll have access to company perks and benefits that'll reward you for the great work you do. We’re growing, fast, and for you that means many exciting opportunities to develop your career with us at Telstra.

Interested?

If you're excited about the opportunity to be part of a team, committed to delivering amazing experiences for our customers – your next step is to apply!

We’re committed to building a diverse and inclusive workforce. To enable everyone to participate, we’ve developed an ‘All Roles Flex’ policy to consider flexible ways of working for every role. To learn more, visit our Telstra Careers Website: tel.st/allrolesflex

We’re committed to building a diverse and inclusive workforce. To enable everyone to participate, we’ve developed an ‘All Roles Flex’ policy to consider flexible ways of working for every role. To learn more, visit http://tel.st/allrolesflex. We welcome applications from Indigenous Australians, people from diverse cultural and linguistic backgrounds and people living with a disability. We encourage you to talk to us about how we can support you through the recruitment process.",Data Engineer,Bengaluru,3.7,$10+ billion (USD),"INR  11,62,278",Telecommunications,10000+ Employees,Company - Public
"Micron Technology, Inc.
3.5",-1,1978,-1,Computer Hardware & Software,"Req. ID: 145252

Recruiter: NIRMAL BEHERA

IT Big Data Operations Engineer

Do you want to be at the forefront of data warehousing and analytics capabilities? Do you accept the challenge of managing and operating with in-memory technologies that deliver game changing business value?

Micron Technology is hiring Big Data data operations engineers to be part of our IT Data Operations Team within the Enterprise Analytics and Data organization. The Data Operation team works closely with development team and Micron’s business functions in all aspects of data, data warehousing, and business intelligence supporting and maintaining data assets and Big Data applications and solutions. We are looking for individuals with Big Data technologies, datawarehousing, database and ETL development skills to be part of our IT Operations team. This role will work primarily with Hadoop, Teradata, SSIS, NiFi and other related Big Data technologies.

Responsibilities and Tasks:
Provide L2 support for Data Warehouse and Business Intelligence (BI) solutions and platforms
Participate in design, architecture review and deployment of Data Warehouse data models and solutions.
Collaborate with business partners and other IT teams to ensure data solutions are available, recover from failures and operate healthy
Troubleshoot operational issues, escalations and resolve business partner issues in a timely manner with strong collaboration and care for business priorities.
Manage knowledge as it relates to Operations.
Own and drive post-mortem for critical issues
Own and support continue improvement plans for solution health and Operational health
Build critical insights to Operations team's performance and operate in a analytic driven operational environment
Work in a shift based rotational schedule
Ability to learn and use mutliple utilities and tools that help with Operations monitoring and alerting
Participate and learn new solutions through condensed knowledge transfer sessions
Participate in the on-call rotation for 24x7 support.
Maintain support documentation, including: FAQs, Knowledge Articles, On-call Guides, Run Books, etc.
Qualifications and Experience
0-3 years of relevant work experience in Data Warehousing, Business Intelligence, and Analytics.
0-3 years of experience working with databases, Hadoop, Teradata and other Big Data ETL technologies
0-3 years Linux/Unix administration skills
Software Engineering background and experience a plus.
Data Vault, Inmon, or Kimball data architecture knowledge is a plus
A passion for data and information with strong analytical, problem solving, and organizational skills.
Preferred qualifications and experience
Experience with various technologies, frameworks and processes, such as Hadoop, Tableau, Grafana, Splunk, data warehousing, data flow mapping, requirements gathering, testing and data validation techniques.
Big Data technology stack administration skills (such as NiFi)
The ability to work in a dynamic, fast-paced, work environment.
Self-motivated with the ability to work under minimal supervision.
Prior experience working in ITIL based environment is expected
ITSM certification is a plus
Education:
B.S. in Computer Science, Management Information Systems, or related fields
We recruit, hire, train, promote, discipline and provide other conditions of employment without regard to a person's race, color, religion, sex, age, national origin, disability, sexual orientation, gender identity and expression, pregnancy, veteran’s status, or other classifications protected under law. This includes providing reasonable accommodation for team members' disabilities or religious beliefs and practices.

Each manager, supervisor and team member is responsible for carrying out this policy. The EEO Administrator in Human Resources is responsible for administration of this policy. The administrator will monitor compliance and is available to answer any questions on EEO matters.

To request assistance with the application process, please contact Micron’s Human Resources Department at 1-800-336-8918 (or 208-368-4748).

Keywords: Hyderabad || Andhra Pradesh (IN-AP) || India (IN) || SGA || Experienced || Regular || Information Systems and Technology || #LI-INHYD1 || Tier 4 ||",Big Data Operations Engineer,Hyderabad,3.5,$10+ billion (USD),"INR 998K - INR 2,191K",Information Technology,10000+ Employees,Company - Public
"NetApp, Inc.
4.2",-1,1992,-1,Enterprise Software & Network Solutions,"42733

Job Summary
You will be part of Enterprise Data & Analytics team responsible for identifying analytical needs, exploring new technologies, and applying data sciences/machine learning concepts to maximize value from data assets. Senior Data Engineer will work closely with key stakeholders both IT and Business to turn data into critical information and knowledge that can be used to make sound business decisions. The individual must have an in-depth understanding of the business environment, an interest in going beyond the obvious, aptitude for new tools/technologies, and obsession for customer success

Essential Functions
Organize, lead, and facilitate multiple teams on highly complex, cross-functional, enterprise data and analytics initiatives
Develop and maintain scalable data pipelines and build out new integrations to support continuing increases in demand for various types of data
Collaborate with key stakeholders to define KPI and build data metrics to measure KPI’s
Define and develop cognitive solutions for business needs, incorporating technologies in Machine Learning, AI, and Analytics
Identify, evaluate and suggest new technology opportunities (including open source) that will have an impact on the enterprise-wide BI systems, machine learning, and predictive analytics
Work with vendors to provide cost estimates and general project management guidance
Job Requirements
Must possess strong subject matter expertise in at least two domains of Sales, Marketing, Install Base, Finance, and Customer Support areas.
Data modeling experience in Enterprise Data Warehouse and DataMart
Hands-on experience in SQL, Python, NoSQL, JSON, XML, SSL, RESTful APIs, and other related standards
Hands-on emphasis with a proven track record of building and evaluating data pipes, and delivering systems for final production
Exposure to Big Data Analytics (data and technologies), Data Sciences, predictive analytics, modelling, machine learning, in-memory applications
Experience with various data systems like Oracle Data Warehouse, SAP HANA, Hadoop/Hive, Vertica, Redshift, Presto, Mangodb
Strong understanding devops, on-premise, and cloud deployments - AWS, Google, Azure
Education
Minimum of 8 to 12 years of experience with Bachelor of Science Degree in Computer Science, Management Information Systems, or Business, or related field is required

Job Segment:
Database, Warehouse, ERP, Developer, Engineer, Technology, Manufacturing, Engineering

Apply now »",Data Engineer,Bengaluru,4.2,$5 to $10 billion (USD),"INR  13,19,498",Information Technology,10000+ Employees,Company - Public
"Jumio Inc
3.9",-1,2010,-1,Enterprise Software & Network Solutions,"Role Title: Cloud Engineer

Reporting to: IT Manager

Location: Jaipur/Udaipur

Role Purpose: To accelerate and serve the business. Enable Jumio to grow and improve by executing on Infrastructure, Network, Security, Operating System, and Software projects. This role is a combination of data center engineer, troubleshooter, sys admin, Server Administrator, project contributor and when needed project lead, system maintenance, future planning and capacity, migrations and moves. All this, in providing world class IT services in a timely, efficient and professional manner.

Jumio’s cloud infrastructure including; ISPs, LAN/WAN/P2P, software, hardware, networks, vendors, SaaS solutions and Cloud environment / instances.

Role Description:
Design, build and configure applications to meet business process and application requirements.
Document Work - quality documentation is an integral part of the role. The Infrastructure Engineer documents system configurations, updates to the system and any new procedures or policies created. Documentation is used by colleagues to understand changes to the system. Work is recorded in JIRA and documentation of workflows, solution methods, SOPs, procedures and site infrastructure is kept in Confluence.
Project Management - the Infrastructure Engineer often collaborates with IT support staff and coordinates with vendors and internal users when implementing new projects. Projects are listed as ‘Epics’ in JIRA. Responsible for setting up meetings, presenting information, ordering supplies and equipment and ensuring projects are delivered on time. Tracking and communicating progress effectively.
Additional Ad Hoc Duties - as assigned, based on business need.
Role Value: The Infrastructure Engineer is integral to the effectiveness of the business. IT serves the business through ensuring peak productivity and performance of the local teams and actively contributing to global projects. Jumio’s IT function is committed accomplishing The Mission: (1) Accelerate the Business and (2) Secure the Business.

Key Responsibilities:
Working in tandem with our engineering team to identify and implement the most optimal cloud-based solutions for the company.
Planning, designing and developing cloud-based applications.
Managing cloud environments in accordance with company security guidelines.
Deploying and debugging cloud initiatives as needed in accordance with best practices throughout the development lifecycle.
Educating teams on the implementation of new cloud-based initiatives, providing associated training as required.
Employing exceptional problem-solving skills, with the ability to see and solve issues before they snowball into problems.
Using your extensive knowledge of APIs to design RESTful services, and integrate them with existing data providers, using JSON or XML as needed.
Lead and develop best practices for larger Cloud Engineer team.
Building and designing web services in the cloud, along with implementing the set-up of geographically redundant services.
Orchestrating and automating cloud-based platforms throughout the company.
Stay current with industry trends, making recommendations as needed to help the company excel.
Technical Experience:
Experience with multiple operating systems, including UNIX, Linux, and Windows 2008 and 2012
Experience with multi-threaded, Big Data, and distributive Cloud architectures and frameworks
Experience with AWS ELB, EC2, VPC, ECS or EKS, ECR, S3, SQS, SNS, SSM, Secrets Manager, KMS, RDS, IAM, AutoScaling, CloudWatch, CloudTrail, Config, Inspector, and WAF.
Experience with building, delivering, and managing Cloud IaaS environments using IaaS platforms, including Amazon Web Services (AWS)
Possession of excellent leadership skills to be used for consensus building and mentoring.
Experience working in AWS via the CLI and management console.
Experience monitoring AWS instances and services.
Experience with AWS platform capabilities, platform architectures, and platform engineering solutions within multiple Cloud accounts and services.
Maintain, optimize, and embrace ownership of ELK (Elastic, Logstash, Kibana) logging stack implementations in cloud.
Migration experience with ELK stack from in-house to AWS cloud.
Strong understanding of JSON data ingest.
Professional Attributes:

Take part in 24x7 Shift Rotations including Morning, Evening and Night Shifts Work out of hours evenings, nights, weekends, bank holidays at reasonable request to perform production changes Travel to other locations when required Able to WFH occasionally Be flexible in the work a platform

Key Characteristics and Attitudes
Mature Big picture and the detail
Self-starting and managing Collaborative and collegiate
Urgency and ownership Humble and helpful
Resilient and tenacious Positive attitude and proactive
Measures that Matter

Keeping projects on time and within budget are key metrics for the Infrastructure Engineer role. Other measures for individual and team performance are taken from varied sources, including but not limited to, JIRA.

The Team

Infrastructure engineers work as part of Infrastructure IT team (currently 5 globally) and report to the IT Manager.

Helping teammates in both day to day and project work happens daily.

The function is committed to becoming a world-class team and moves fast, with intensity and accuracy.

Integrity and team cohesion are valued by all team members who subscribe to the belief that Team > Teamwork > Self.

The team is globally distributed. There will be Team Meetings and Project Meetings that are early in the morning and late at night due to the global distribution of colleagues.

Progression

In a rapidly expanding business opportunities for progression are frequent and unlimited. With new sites, growing teams and a license to explore the latest technologies there is no practical limit on progression.

@Work

Jumio has sites in India, London, Palo Alto, Vienna

Company

Jumio is the future for online and mobile ID verification. We are the largest and fastest growing company in the ID verification space. With a global footprint, we’re expanding the team to meet strong client demand across a range of industries including Financial Services, Travel, Sharing Economy, Fintech, Gaming, and others.

Equal Opportunities

Jumio is a collaboration of people with different ideas, strengths, interests and cultures. We welcome applications and colleagues from a wide range of backgrounds and statuses.",Cloud Engineer,Jaipur,3.9,$100 to $500 million (USD),INR 21K - INR 28K,Information Technology,201 to 500 Employees,Company - Private
"Crimson Interactive Pvt. Ltd.
2.9",-1,2005,-1,Consulting,"We are building a world-class language-related product that has the potential to positively transform lives worldwide. We have a passionate team of data scientists, coders, and linguists who have been working on it.

We are looking for a Data Engineer who will identify and implement the best data-driven methodologies considering the product requirements.

Work location: Goregaon (West), Mumbai

Key responsibilities:

Create, build and design data management systems across an entire organization
Work with very large data sets (both structured and unstructured).
Help data scientist to easily retrieve the needed data for their evaluations and experiments.
Design, develop and implement R&D and pre-product prototype solutions
Must have strong engineering skills that will help engineering team to productivize NLP/ML algorithms
Implement scalable, maintainable, well documented and high-quality solutions.
Stay abreast of the new developments in Artificial Intelligence (AI)/Machine Learning (ML). Contribute to the research strategy and technical culture of the team


Skills Required:

BTech/MTech/ME/MCA from reputed Engineering college.
0-2 years of industry experience
Extremely curious and relentless at figuring out solutions to problems
Knowledge of Big Data platforms like Hadoop and its eco-system
Proficiency in programming languages like Java/C/C++/Python
Experience with cloud services
Exposure to NLP and its related services.
Experience with one or more visualization tools like Tableau, etc.
Experience with Docker, Kubernetes, Kafka, Elasticsearch, Lucene
Experience with relational or NoSQL databases such as MySQL, MongoDB, Redis, Neo4j.
Experience of handling various data types and structures: structured and unstructured data, validating and cleaning data, and measuring evaluation
Excellent understanding of machine learning techniques.",Data Engineer,Mumbai,2.9,Unknown / Non-Applicable,"INR  26,156/mo",Business Services,201 to 500 Employees,Company - Private
"Amazon.com, Inc.
4.3",-1,1994,-1,Internet,"Amazon.com was recently voted #2 most admired company in the US, #1 most innovative, and # 1 in Customer Service. We are investing heavily in building an excellent advertising business, and are responsible for defining, and delivering a collection of self-service performance advertising products always-on analytics that is fully scalable and reliable. Our products are strategically important to our leadership, finance, economists, analysts, and BI partners to drive long-term growth. We mine billions of ad impressions and millions of clicks daily and are breaking fresh ground to create world-class products. We are highly motivated, collaborative, and fun loving with an entrepreneurial spirit and bias for action. With a broad mandate to experiment and innovate, we are growing at an unprecedented rate with a seemingly endless range of new opportunities.
The Advertising Analytics and Data Management team is looking for an exceptional Data Engineer who is passionate about data and the insights that large amounts of data can provide, who thinks/acts globally, and who has the ability to contribute to major novel innovations in the industry. The role will focus on working with a team of data engineers, business and tech savvy professionals to lay down scalable data architecture to ingest large amounts of structured and unstructured datasets and work with stakeholders to drive business decisions based on these datasets.

The ideal candidate will possess both a data engineering background and a strong business acumen that enables him/her to think strategically and add value to the customer experience. He/She will experience a wide range of problem solving situations, requiring extensive use of data collection and analysis techniques such as data mining and machine learning.

The successful candidate will work with multiple global site leaders, Business Analysts, Software Developers, Database Engineers, Product Management in addition to stakeholders in sales, finance, marketing and service teams to create a coherent customer view. They will:

· Develop and improve the current data architecture using AWS Redshift, AWS S3, AWS Aurora (Postgres) and Hadoop/EMR.
· Improve upon the data ingestion models, ETL jobs, and alarming to maintain data integrity and data availability.
· Stay up-to-date with advances in data persistence and big data technologies and run pilots to design the data architecture to scale with the increased data sets of advertiser experience.
· Partner with analysts across teams such as product management, operations, sales, finance, marketing and engineering to build and verify hypothesis to improve the business performance.
· Manage weekly business reports via dashboards and paper the analyses of daily, weekly, and monthly reporting of performance via Key Performance Indicators.



Basic Qualifications

· 3+ years of experience as a Data Engineer or in a similar role
· Experience with data modeling, data warehousing, and building ETL pipelines
· Experience in SQL
· 3+ years of experience as a Data Engineer or in a similar role
· Experience with data modeling, data warehousing, and building ETL pipelines
· Experience in SQL



Preferred Qualifications

· AWS Experience
· Advertising domain knowledge is a plus

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

#sspajobs",Data Engineer,Bengaluru,4.3,$10+ billion (USD),"INR  14,39,361",Information Technology,10000+ Employees,Company - Public
"Groupon, Inc.
3.4",-1,2008,-1,Internet,"Can data really help local businesses around the world thrive?

Are you ready to test your skills using massive amounts of information to make critical business decisions?

Groupons mission is to become the daily habit in local commerce and fulfill our purpose of building strong communities through thriving small businesses by connecting people to a vibrant, global marketplace for local services, experiences and goods. In the process, were positively impacting the lives of millions of customers and merchants globally. Even with thousands of employees spread across multiple continents, we still maintain a culture that inspires innovation, rewards risk-taking and celebrates success. If you want to take more ownership of your career, then you're ready to be part of Groupon.

As a part of our Commercial organization, our Revenue Management Analytics and Platform teams strive to be the voice of the customer in the decisions Groupon makes. We develop and use cutting-edge data technologies to sift through large amounts of data and distill it into information about our customers' wants and needs. We then integrate these insights into our automated decision-making systems to constantly improve our services.

As a Data Engineer you will be part of our Revenue Management Analytics Data COE, tasked with improving the data and reporting infrastructure that guides decision-making across Marketing, Merchandising, Sales & Ops, Product, and the broader Revenue Management team. You will partner closely with the Analytics team to create data-driven solutions to business problems. The ability to conceptualize and create user-friendly self-service solutions is critical to be successful in this role. Our business is evolving quickly and we need you to think long term, but deliver incrementally.

We're a ""best of both worlds"" kind of company. We're big enough to have resources and scale, but small enough that a single person has a surprising amount of autonomy and can make a meaningful impact. We're curious, fun, a little intense, and kind of obsessed with helping local businesses thrive. Does that sound like a compelling place to work?

Youll spend time on the following:
Developing and maintain new data sources that help drive the business
Architecting a new modern event driven architecture to fit AWS processing model (EMR + Spark, S3)
Working on Big Data technologies including Hadoop/Hive, PySpark, and Presto
Other technologies youll get to work with include Snowflake for reporting and Airflow for orchestration
Operating in an agile model leveraging scrum methodologies and Jira tools
Migrating existing data pipelines from on-prem regional data centers to AWS cloud
Leading the entire software lifecycle including hands-on development, code reviews, testing, deployment, and documentation for batch ETL's
Were excited about you if you have:
Bachelors degree in Computer Science or equivalent with substantial data engineering experience
At least 3+ years of recent hands-on SQL programming experience in a Big Data environment is required; Hadoop/Hive experience is preferred
3+ years of recent hands-on experience with a modern programming language (Scala and/or Python) is required; Spark/Pyspark is preferred
Experience working in AWS cloud landscape is required
Experience developing and maintaining ETL applications and data pipelines using big data technologies is required; Airflow experience is a plus!
Ability to manage multiple stakeholders with excellent communication and interpersonal skills is a must
Understanding of full software development life cycle, agile development and continuous integration
Excellent understanding of data warehousing and modeling concepts (E.g., Star and snowflake schemas)
Ability to model data and and develop reporting solution from scratch
Prior e-commerce experience is a big plus


Groupon's purpose is to build strong communities through thriving small businesses.

To learn more about the world's largest local ecommerce marketplace,
click here for the latest Groupon news.
Plus, be sure to

check out the values

that shape our culture, guide our strategy and make our company a great place to work. And just don't take our word for it.

Hear from real Groupon team members

and learn more about our inclusive employee groups. If all of this sounds like something that's a great fit for you, then click apply and let's see where this takes us.

Groupon is an Equal Opportunity Employer

Qualifications for employment, promotion, and other terms and
conditions of employment are based upon the ability to perform the job.
Equal-employment opportunities are provided to all applicants and
employees without regard to race, creed, religion, color, age, national
origin, sex, disability, medical condition, sexual orientation, gender
identity or expression, genetic information, ancestry, marital status,
military discharge status (excluding dishonorable discharge), veteran
status, citizenship status, or other legally protected status. We are
all responsible for maintaining this policy. Groupon is committed to
providing reasonable accommodations for qualified individuals with
disabilities and disabled veterans in our job application procedures. If
you need assistance or an accommodation due to a disability, you may
email us at hraccommodations at groupon.com. If you have concerns
related to Groupon’s equal employment opportunities, you may contact
Groupon's Ethics Reporting Service Ethicspoint.",Data Engineer,Bengaluru,3.4,$1 to $2 billion (USD),"INR  3,14,141",Information Technology,5001 to 10000 Employees,Company - Public
"VMware, Inc.
4.3",-1,1998,-1,Computer Hardware & Software,"At VMware, we are committed to helping our people grow professionally. Our talented employees exemplify our shared values and continue to drive our company to new heights. If you see a position that might be right for you, we encourage you to apply and continue to be a part of our EPIC2 community.

VMware is the leader in virtualization and cloud infrastructure solutions that enable our more than 350,000 enterprise and SMB customers to thrive in the Cloud era. A pioneer in the use of virtualization and automation technologies, VMware simplifies IT complexity across the entire data center to the virtual workplace, empowering customers with solutions in the software-defined data center to hybrid cloud computing and the mobile workspace.

Our team of 20,000+ people working in 50+ locations worldwide is committed to building a community where great people want to work long term by living our values of passion, innovation, execution, teamwork, active learning and giving back. If you are ready to accelerate, innovate and lead, join us as we challenge constraints and problem solve for tomorrow today.

Job Description

VMware Data Engineering team is seeking a highly motivated, experienced Big Data Engineer within Data & Analytics group located in Bangalore, India. This position is responsible for hands on development work on all aspects of Big Data, data provisioning, modeling, performance tuning and optimization. The candidate will work closely with both Enterprise and Solution Architecture teams to translate the Business/Functional requirements into technical specifications that drive Big Data solutions to the meet functional requirements.

Qualifications & Skills
Bachelors degree or equivalent required with preference for Computer Science or Engineering. MS degree would be highly desirable
Big Data consultant/ engineer with at least 3+ years of overall experience and hands-on experience in Hadoop - MapReduce, HDFS, Hbase, Hive, Sqoop, MongoDB, NoSQL or Cassandra.
Hands on Experience with Cloudera Distribution & expertise in Spark & Python/Scala/Go (any one).
Experience with Kafka and Presto would be highly desirable.
Proven expertise in a wide variety of database technologies, from Postgres, SQLServer to NoSQL systems such as MongoDB, Cloudant, Cassandra, and/or Elasticsearch, and can explain their varied use cases
Desired Qualifications:

- - Familiarity with SAP HANA Information Models within the Enterprise HANA platforms

- - Knowledge of Data Integration Platforms- Informatica PowerCenter, SAP BODS , SDI, SLT

- -Experience with Cloud technologies

Location - Bangalore, India

Category : Engineering and Technology
Subcategory: Software Engineering
Experience: Manager and Professional
Full Time/ Part Time: Full Time
Posted Date: 2020-07-27

VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape whats possible today at http://careers.vmware.com.

Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",Big Data Engineer,Bengaluru,4.3,$5 to $10 billion (USD),"INR  15,12,527",Information Technology,10000+ Employees,Company - Public
"GlaxoSmithKline plc
3.9",-1,1830,-1,Biotech & Pharmaceuticals,"Site Name: India - Karnataka - Bangalore
Posted Date: Jul 21 2020

GSK is one of the worlds foremost pharmaceutical and healthcare companies and we are proud to be leading a healthcare revolution.

By disrupting our approaches to R&D and commercial business processes, D&A is allowing us to integrate, simplify and unlock all our data to drive innovation, decision making and enable our transformation in servicing our patients, healthcare professionals and consumers.

This role will provide YOU the opportunity to lead key activities to progress YOUR career. These responsibilities include some of the following.
Development: You believe in hands on, development and delivery.
Delivery: Ensure project goals are achieved on time in alignment with the stakeholders expectation. Ability to work on complex projects and in a distributed environment. Escalate to other Data & Analytics leadership team when needing support. Work in close collaboration with other team members in the CH BI Tech team, to ensure Development/Delivery aspects are well represented in the projects requirements and deliverables.
Methodology: Incorporate agile ways of working into the delivery process thru use of DABL (Discovery, Alpha, Beta, Launch) framework to show value periodically. Individuals will work as part of product-centric delivery team(s) that will focus on delivering value independently while fully embracing integrated DevOps approaches.
Ownership: Take ownership for the delivery/development projects and help steer until completion.
Governance: Follow governance that allows projects and stakeholders to manage overall project performance and manage programme risks within the global nature of some of the programmes.
Forward looking: Remain flexible towards technology approaches to ensure that the best advantage is being taken of new technologies. Keep abreast of industry developments in analytics and be able to interpret how these would impact services and present new opportunities.
Quality, Risk & Compliance: Ensure all risk and issues associated with owned projects are recorded and managed in the appropriate Risk & Issue logs in a timely manner. Ensure all Risks and Issues have clear action/mitigation/contingency plans defined, with named action owners and timelines for completion.
Technical Architecture: Be conversant with technical architecture to contribute to design discussions in partnership with the Delivery/Development Director and dedicated Analytics & Data Architect.
We are looking for a data artist and if you have these skills, we would like to speak to you.
Experience as a Developer in the Data & Analytics arena with demonstrated expertise in emerging technologies and data technology platforms and management.
MS/BS degree in Computer Science, Engineering, Design or equivalent experience.
Ideal candidate would have built an impressive hands-on career to date in an advanced, recognized, and innovative environment around Data & Analytics.
Must have worked in CI/CD ways of working using tools like Azure DevOps.
Fully conversant with big-data processing approaches and schema-on-read methodologies are a must and knowledge of Azure Data Factory/DataBricks/Azure Data Lake/Azure DW/Analysis Services is a must.
Experience of the Azure analytics components, Power BI, Power Apps & Microsoft Visual Studio is desirable.
Good to have an excellent development skills & extensive hand-on development & coding experience in a variety of languages, e.g. C#, Python, SQL, DAX, etc.
Have a highly innovative mind-set and experience with analytics in a healthcare or CPG company.
Ability to work in close partnership with other IT functions such as IT security, compliance, infrastructure, etc. as well as partner closely with business stakeholders in the commercial and digital organizations.
Experience in executing Data Analytics projects in an Agile manner, articulation of Value depending on the project life cycle stage, Creating MVPs, developing plans for scale up are all very important experience to be successful in this role.
Great communication skills and ability to communicate inherently complicated technical concepts to non-technical stakeholders.
Why GSK?


Our values and expectations are at the heart of everything we do and form an important part of our culture. These include Patient focus, Transparency, Respect, Integrity along with Courage, Accountability, Development, and Teamwork. As GSK focuses on our values and expectations and a culture of innovation, performance and trust, the successful candidate will demonstrate the following capabilities.

GSKIndia_DA

*LI-GSK

Our goal is to be one of the worlds most innovative, best performing and trusted healthcare companies. We believe that we all bring something unique to GSK and when we combine our knowledge, experiences and styles together, the impact is incredible. Come join our adventure at GSK where you will be inspired to do your best work for our patients and consumers. A place where you can be you, feel good and keep growing.

Important notice to Employment businesses/ Agencies

GSK does not accept referrals from employment businesses and/or employment agencies in respect of the vacancies posted on this site. All employment businesses/agencies are required to contact GSK's commercial and general procurement/human resources department to obtain prior written authorization before referring any candidates to GSK. The obtaining of prior written authorization is a condition precedent to any agreement (verbal or written) between the employment business/ agency and GSK. In the absence of such written authorization being obtained any actions undertaken by the employment business/agency shall be deemed to have been performed without the consent or contractual agreement of GSK. GSK shall therefore not be liable for any fees arising from such actions or any fees arising from any referrals by employment businesses/agencies in respect of the vacancies posted on this site.

It has come to our attention that the names of GlaxoSmithKline or GSK or our group companies are being used in connection with bogus job advertisements or through unsolicited emails asking candidates to make some payments for recruitment opportunities and interview. Please be advised that such advertisements and emails are not connected with the GlaxoSmithKline group in any way.

GlaxoSmithKline does not charge any fee whatsoever for recruitment process. Please do not make payments to any individuals / entities in connection with recruitment with any GlaxoSmithKilne (or GSK) group company at any worldwide location. Even if they claim that the money is refundable.

If you come across unsolicited email from email addresses not ending in gsk.com or job advertisements which state that you should contact an email address that does not end in gsk.com, you should disregard the same and inform us by emailing askus@gsk.com, so that we can confirm to you if the job is genuine.",Data Engineer,Bengaluru,3.9,$10+ billion (USD),"INR  11,13,232",Biotech & Pharmaceuticals,10000+ Employees,Company - Public
"International Business Machines Corporation
3.9",-1,1911,-1,IT Services,"Introduction
At IBM, work is more than a job - it's a calling: To build. To design. To code. To consult. To think along with clients and sell. To make markets. To invent. To collaborate. Not just to do something better, but to attempt things you've never thought possible. To lead in this new era of technology and solve some of the world's most challenging problems.

Your Role and Responsibilities
As a Data Engineer, you play a vital role in building the right infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Expert in setting up effective pipelines to capture data from multiple sources into the enterprise centric storage.
Comfortable in building effective analytical tools that utilize the data pipeline to provide actionable insights into data synchronization, reporting, operational efficiency and related areas.
Work with stakeholders including the product owner, data and design teams to assist with data-related technical issues and support their data infrastructure needs.
Create and maintain optimal data pipeline architecture.
Identify, design, and implement process improvements aimed at automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Assemble large, complex data sets including legacy structured data warehouse that meet functional / non-functional business requirements.
Collaborate with DevOps team to develop Continuous Integration/Continuous Delivery pipelines using containerization technologies.
Solve Big Data and Distributed Data Streaming problems using latest technologies.
Perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Manipulate, process and extract value from large disconnected datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management
Work on State-of-the-Art cloud technologies provided by IBM Public Cloud, RedHat, AWS & others.
Be part of open, transparent agile teams who always thrive for continuous learning and contribute towards continuous improvement.
Required Technical and Professional Expertise
Possess strong knowledge in designing database models to store structured & unstructured data efficiently and in creating effective data tools for analytics experts.
Knowledge in technologies like Hadoop, Spark, Kafka, Scala, Python, etc. Knowledge in relational model databases (like DB2, MySQL, Oracle, ...) and no-SQL databases (MongoDB, Elastic Search, ...)
Knowledge on enterprise data lakes, data analytics, reporting, in-memory data handling, enterprise integration tools, etc.
Good understanding of industry best practices for data governance and security.
Good communication skills and fluent in English.
Preferred Technical and Professional Expertise
None
About Business Unit
We at IBM Chief Information Office (CIO) are a dynamic group of Business, Strategy and Technology professionals - a specific source of market-leading Industry Consulting, Application and Business process delivery following Agile values and principles. CIO is at the forefront of Digital Reinvention of key applications used within IBM providing value-led and asset-powered end to end solutions.

CIO mission is to create a productive environment for everyone at IBM. We do this by leading with Design to drive simplicity and ease of use, Engineering the systems that run the business, and Innovating to transform the business. Key focus areas are to secure the Enterprise in network security, endpoint security and data security
Transform IBM and improve collaboration and practice a culture of agile way of working.

Within the CIO, we represent the Analytic Solutions area of the Sales & Marketing Systems. Our mission is to provide business insights to our partners in sales and marketing to win in the market place through analytics solutions delivered using latest technology and based on real time, consolidated and cloud-based data.

Your Life @ IBM
We at IBM Chief Information Office (CIO) are a dynamic group of Business, Strategy and Technology professionals - a specific source of market-leading Industry Consulting, Application and Business process delivery following Agile values and principles. CIO is at the forefront of Digital Reinvention of key applications used within IBM providing value-led and asset-powered end to end solutions.

CIO mission is to create a productive environment for everyone at IBM. We do this by leading with Design to drive simplicity and ease of use, Engineering the systems that run the business, and Innovating to transform the business. Key focus areas are to secure the Enterprise in network security, endpoint security and data security
Transform IBM and improve collaboration and practice a culture of agile way of working.

Within the CIO, we do represent the Analytic Solutions area of the Sales & Marketing Systems. Our Mission is to provide business insights to our partners in sales and marketing to win in the market-place through analytics solutions delivered using latest technology and based on real time, consolidated and cloud based data.

About IBM
IBMs greatest invention is the IBMer. We believe that progress is made through progressive thinking, progressive leadership, progressive policy and progressive action. IBMers believe that the application of intelligence, reason and science can improve business, society and the human condition. Restlessly reinventing since 1911, we are the largest technology and consulting employer in the world, with more than 380,000 IBMers serving clients in 170 countries.

Location Statement
For additional information about location requirements, please discuss with the recruiter following submission of your application.

Being You @ IBM
IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.",Data Engineer,Bengaluru,3.9,$10+ billion (USD),"INR  3,90,863",Information Technology,10000+ Employees,Company - Public
"Crayon Data
4.4",-1,2012,-1,Enterprise Software & Network Solutions,"Who are we?

A young, fast-growing AI and big data company, with an ambitious vision to simplify the world’s choices. Our clients are top-tier enterprises in the banking, e-commerce and travel spaces. They use our core AI-based choice engine maya.ai, to deliver personal digital experiences centered around taste. The maya.ai platform now touches over 125M customers globally. You’ll find Crayon Boxes in Chennai and Singapore. But you’ll find Crayons in every corner of the world. Especially where our client projects are – UAE, India, SE Asia and pretty soon the US.

Life in the Crayon Box is a little chaotic, largely dynamic and keeps us on our toes! Crayons are a diverse and passionate bunch. Challenges excite us. Our mission drives us. And good food, caffeine (for the most part) and youthful energy fuel us. Over the last year alone, Crayon has seen a growth rate of 3x, and we believe this is just the start.

We’re looking for young and young-at-heart professionals with a relentless drive to help Crayon double its growth. Leaders, doers, innovators, dreamers, implementers and eccentric visionaries, we have a place for you all.

Can you say “Yes, I have!” to the below?
Expertise in data warehousing, relational database architectures (Oracle, SQL, DB2, Teradata), and big data storage and processing platforms required. (Hadoop, HBASE, Hive, Spark)
Working knowledge of cloud-based deployments in AWS, Azure or GCP
Technical know-how of at least one programming stack - ideally Java
Can you say “Yes, I will!” to the below?
Design algorithms for product development and build analytics-based products
Lead analytical projects and deliver value to customers
Coordinate individual teams to fulfill client requirements and manage deliverables
Communicate and present complex concepts to business audiences
Manage and strategize business from an analytics point of view
Travel to client locations when necessary
Requirements

You’ll get brownie points for:
An aptitude for analytical problem solving
Comfortable working with Linux and Shell
The ability to work effectively in a global team and thrive in a fast-paced, evolving tech startup environment",Data Engineer,Chennai,4.4,Unknown / Non-Applicable,INR 17K - INR 18K,Information Technology,51 to 200 Employees,Company - Private
"Jumio Inc
3.9",-1,2010,-1,Enterprise Software & Network Solutions,"Role Purpose:
The Solutions Expert should be able to provide technical, functional, and subject matter expertise. Research and diagnose the customer data in order to provide them great experience and add value throughout their lifetime with Jumio. Also, help customers understand how to get the best out of Jumio’s products.

Responsibilities:
Understanding the intended use-case and propose recommendations on how best to implement Jumio’s solution to meet customer requirements
Provide technical, functional, and subject matter expertise to Jumio’s Customer Success team.
Create and present a detailed analysis and technical presentations for customers, outlining their problems, opportunities and solutions.
Drive projects in coordination with different teams and track progress, resolving problems; publishing progress reports; recommending actions.
Extract, interpret, evaluate and interrelate data to develop integrated business analysis and projections, which is to be incorporated into the business strategic decision-making
Perform daily, weekly and monthly reviews and analyses of current processes using operational metrics and reports
Actively participate in the implementation of approved changes in process/product and/or customers.
Create informative, actionable and repeatable reporting that highlights relevant business trends and opportunities for improvement.
Creating and delivering customized demos on different platforms (web, iOS, Android)
Conduct insightful, ad hoc analyses to investigate ongoing or one-time operational issues.
Manage prospect testing experience by submitting batch upload processing, coordinating with operations and also working out scope and requirements with operations.
Nurse customer accounts to support speedy implementation and ensure healthy conversion rates.
Develop clear understanding and address issues of key account needs.
Testing of new features/products released and share feedback to concerned teams from a customer viewpoint.
Experience and Qualifications:
Bachelor’s Degree in appropriate field of study or equivalent work experience
Strong customer facing experience, used to interacting with all levels and departments across enterprise customers
Advance Microsoft Office skills, specifically Microsoft Excel and PowerPoint.
Basic technical knowledge about programming languages, mobile operating systems (iOS & Android)
Minimum working experience : 3 years.
In depth technical knowledge, including web and mobile programming or scripting languages (including, but not limited to Java, Objective C, .NET, Swift, XML, PHP), computer operating systems (Unix, Windows), mobile operating systems (iOS, Android)
Great to have Experience and Qualifications:
Existing knowledge of the ID verification, AML or KYC market and the competitive landscape
Knowledge of user digital on boarding, payment and/or risk management systems
Degree education in a relevant field e.g. Computer Science, Engineering
A ‘customer success’ mentality, able to make using Jumio’s products as painless and positive as possible for customers - leading to increasing volumes of transactions
Experience in JIRA, Confluence, Salesforce, PowerBi is a plus.
Key Characteristics and Attitudes:
In a recent global survey these attributes were valued by Jumios in all locations and functions - we firmly believe in hiring for attitude as well as skill.
Friendly and supportive
Adaptable and flexible
Articulate and persuasive
High IQ and EQ
Curious and coachable
Commercially Aware
Resilient and tenacious
Big picture and the detail
Jumio Values:
IDEAL: Integrity, Diversity, Empowerment, Accountable, Leading Innovation

@Work:
Our newest office, Jumio is next to Walmart Labs in Helios Business Park and growing fast. A hub of technical excellence with Machine Learning enablement at its core the engineers and team are committed to learning and innovation. Since opening in 2019 the team has grown rapidly with multiple functions now represented.

Company:
Jumio is the future for online and mobile ID verification. We are the largest and fastest growing company in the ID verification space. With a global footprint, we’re expanding the team to meet strong client demand across a range of industries including Financial Services, Travel, Sharing Economy, Fintech, Gaming, and others.

Equal Opportunities:
Jumio is a collaboration of people with different ideas, strengths, interests and cultures. We welcome applications and new colleagues from all backgrounds and of all statuses.",Solutions Expert/Engineer,Bengaluru,3.9,$100 to $500 million (USD),INR 21K - INR 28K,Information Technology,201 to 500 Employees,Company - Private
"Conviva, Inc.
4.1",-1,2006,-1,Internet,"Have you streamed a program on Disney+, caught the latest binge-worthy series on Hulu, or tuned in for NFL highlights on social media? If the answer is yes, you have already benefited from Conviva's technology.

At Conviva, we create the technologies and standards driving industry transformation and streaming media growth worldwide. It's not about the data, it's about what you do with it. As the leader in global streaming media intelligence, Conviva measures in excess of 500 million unique viewers watching 150 billion streams per year with 1.5 trillion real-time transactions per day across more than 180 countries. Our platform and products are designed to deliver the real-time, cross-screen, integrated insights our customers need to understand their business and focus on what matters. Join us as we make streaming better.

What you get to do in this role:

Work on extremely large scale real time big data systems

Design and develop solutions for challenging distributed systems problems

Champion a high bar on code quality and unit testing

Mentor and lead junior developers

What you bring to the role:

10-12 years of hands-on software development experience

Strong CS fundamentals, including system design, data structures and algorithms

Ability to design high quality solutions for distributed systems

High level of proficiency in Java or Scala

Expertise in profiling and troubleshooting the JVM

Expertise in Big Data systems like Hadoop, Kafka, Spark, Flink, Druid, Hbase etc

Good analytical and troubleshooting skills

What will help you stand out:

Experience with real time big data streaming systems

Good Scala and functional programming skills

Experience working with massive data volumes

Experience on Flink or Akka

Conviva is the leader in streaming media intelligence, powered by its real-time platform. More than 250 industry leaders and brands including CBS, CCTV, Cirque Du Soleil, DAZN, Disney+, HBO, Hulu, Sky, Sling TV, TED, Univision, and Warner Media rely on Conviva to maximize their consumer engagement, deliver the quality experiences viewers expect and drive revenue growth. With a global footprint of more than 500 million unique viewers watching 150 billion streams per year across 3 billion applications streaming on devices, Conviva offers streaming providers unmatched scale for continuous video measurement, intelligence and benchmarking across every stream, every screen, every second. Conviva is privately held and headquartered in Silicon Valley, California, with offices around the world. For more information, please visit us at www.conviva.com","Lead Software Engineer, Big Data",Bengaluru,4.1,Unknown / Non-Applicable,"INR 6,025K - INR 6,531K",Information Technology,201 to 500 Employees,Company - Private
"Profectus Solutions Inc.
4.9",-1,2017,-1,"Department, Clothing, & Shoe Stores","What we are looking for:
Must be proficient in advanced working SQL, experience working with a variety of databases and a stronghold in Hadoop (Spark, Spring)/Google Cloud technology (AWS) for extracting and analyzing large datasets
Hands on experience with one or more of Java, Scala or Python language
Experience working in various data warehouses, reporting/ analytic tools and environments
Good to have an exposure to and fundamental understanding of advanced statistical techniques
Knowledge of deep learning and AI tools and their application in the Retail domain will be preferred

Responsibilities:
Design, construct, install, test and maintain highly scalable data management systems

Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and cloud-based ‘big data’ technologies

Create data catalog, data flow diagrams and interprets data results to users

Use data and their analytical ability to find and interpret rich data sources; manage large amounts of data; merge data sources; ensure consistency of datasets; create visualizations to aid in understanding data; aid in building mathematical/statistical models using the data; and present and communicate the data insights/findings and produce and present results with dashboard

You must be:
A team player who likes to work hard and play harder, have excellent interpersonal, organizational and time-management skills

Able to think strategically and analytically to effectively complete assigned work within given timelines

Someone who possesses excellent written and oral communication skills and have an attention to detail

A person with an ability to multi-task on multiple projects and tasks at the same time

A person who gives importance to attention to detail and be highly organized

Positive and upbeat with the ability to learn quickly

Be able to laugh. At others and most importantly at yourself. Need a sense of humor

You can expect:
A fast-paced, high-growth startup environment where you will gain a career and not just a job

The company to invest in your personal and professional development. We support your ongoing education and training by reimbursing you for relevant educational courses

An open office culture, no cabins or cubicles and a place that is looking for your input to help us grow

The support of your teammates to always do better. Own it and win together!

Exposure of International Retail market. Learn about a high growth industry and build critical skill-set

Excellent employee referral program. Refer your friends, work with your friends and be awarded for it

Work along with the smart, creative and energetic team who truly believe in ‘working hard and partying harder!’

Educational Requirement:
UG – B.Tech/B.E.",Data Engineer,Bengaluru,4.9,Unknown / Non-Applicable,INR 513K - INR 857K,Retail,1 to 50 Employees,Company - Private
"Juniper Networks, Inc.
3.8",-1,1996,-1,Telecommunications Services,"About the Group:

The mission of the Juniper Digital Experience and Automation (DEA) team (part of the Juniper Global Services (GS) Organization) is to delight external and internal customers by delivering efficient, innovative solutions that are proactive, easy to use and self-service focused.

DEA is at the heart of the Global Services digital strategy. We buy, build and integrate innovative technologies that enables the Global Services business with a focus on business agility and efficiency. Through partnerships with business subject matter experts we identify need, ideate, determine idea value (ROI) and ultimately realize new capabilities. Automation, Data Science / Machine Learning and Advanced Analytics are critical components of that innovation work.

About the position:

We are looking for a savvy Data Engineer to join our growing DEA Team. The hire will be responsible for building, expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our citizen data scientists, data analysts and other DEA team members on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

Responsibilities for Data Engineer
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Snowflake and AWS ‘big data’ technologies.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Strong communication skills
Required Qualifications:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Strong communication skills with a growth and learning mindset
Comfort working in a dynamic, research-oriented group with several ongoing concurrent projects
Familiarity with git repo and willing to provide DevOps support for a brief period
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with at least 5+ years of experience in a Data Engineer role. The ideal candidate should have experience using the following software/tools:
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Snowflake, Postgres and Cassandra.
Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Preferred Qualifications
MS in Computer Science, Electrical Engineering or other Engineering majors with 7+ years of total experience.
Knowledge and experiences using machine learning frameworks such as NumPy, ScikitLearn, MLlib, Tensorflow
Knowledge and experiences working with Kubernetes
Juniper Networks is an Equal Opportunity/Affirmative Action Employer.

ABOUT JUNIPER NETWORKS

Juniper Networks is in the business of network innovation. From devices to data centers, from consumers to cloud providers, Juniper Networks delivers the software, silicon and systems that transform the experience and economics of networking. Our products and technology run the world’s largest and most demanding networks today, enabling service providers, enterprises, and governments to create value and accelerate business success. Everyday our 9,000+ colleagues come together across 46 countries to realize our company vision – Connect Everything, Empower Everyone. We are innovating in ways that empower our customers, our partners and ultimately, everyone, in a connected world. These customers include the top 130 global service providers, 96 of the Fortune 100 and hundreds of public sector organizations.

WHERE WILL YOU DO YOUR BEST WORK?

Wherever you are in the world, whether it’s downtown Sunnyvale or London, Westford or Bangalore, Juniper is a place that was founded on disruptive thinking – where colleague innovation is not only valued, but expected. We believe that the great task of delivering a new network for the next decade is delivered through the creativity and commitment of our people. The Juniper Way is the commitment to all our colleagues that the culture and company inspire their best work—their life’s work. At Juniper we believe this is more than a job - it is an opportunity to help change the world...",Senior Data Engineer,Bengaluru,3.8,$2 to $5 billion (USD),"INR  18,82,489",Telecommunications,5001 to 10000 Employees,Company - Public
"Amazon.com, Inc.
4.3",-1,1994,-1,Internet,"Amazon has built a reputation for excellence with recent examples of being named #1 in customer service, #1 most trusted, and #2 most innovative. Amazons cloud product line Amazon Web Services (#AWS), is carrying on that tradition while leading the world in Cloud technologies globally. In India, Amazon Internet Services Private Ltd. (AISPL), is the local seller for cloud services (AWS Services) and is looking for an experienced and qualified personnel to join its Premium Support team in India focusing on Amazons AWS referred as AWS Support.

AWS Support provides global technical support to a wide range of customers using AWS Cloud as they build mission-critical applications on top of AWS services. As a member of the AWS Premium Support team, employed with and working for AISPL, you will be at the forefront of this transformational technology assisting a global list of companies that are taking advantage of a growing set of services and features to run their mission-critical applications. You will work with leading companies in this space and directly with the engineering teams within AISPL developing these new capabilities.

As a Senior Cloud Support Engineer you will do the following:
- Able to troubleshoot and resolve complex technical problems.
- Able to review customers architecture and provide guidance
- Develop and deliver training
- Partner with development teams on complex issues
- Participate in recruiting
- Write tools/script to help the team
- Work with leadership on process improvement or strategic initiatives.





Basic Qualifications

BASIC QUALIFICATIONS
§ Bachelors degree in Computer Science or equivalent
§ 10+ years of relevant experience
§ In depth understanding of RDBMS/NoSQL
§ Several years of working experience with OLTP/OLAP/Data modeling
§ Solid understanding of ETL/BI/BI tools
§ Experience with either of the following programming languages: Python/R/Scala/Java
§ Good experience in working with Distributed computing environment and virtualization
§ Demonstrated ability to successfully influence stakeholders with individually developed concepts
§ Strong written, verbal communication and persuasion skills

Preferred Qualifications

PREFERRED QUALIFICATIONS
§ Experience in working with ML models
§ Experience with query analyzers and query tuning / slow query optimization etc.
§ Exposure to security concepts
§ Experience with Monitoring / Troubleshooting tools
§ Experience in managing full application stack from the OS up through custom applications


Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer, and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, disability, age, or other legally protected status.",Sr. Cloud Support Engineer Big Data,Bengaluru,4.3,$10+ billion (USD),"INR  14,39,361",Information Technology,10000+ Employees,Company - Public
"Google Inc.
4.4",-1,1998,-1,Internet,"Due to the current health crisis related to COVID-19 and the escalating visa/travel restrictions in place, we're currently unable to extend offers to anyone who cannot work from India due to lockdown visa/travel restrictions, or other restrictive measures until further notice. Consequently, we will be prioritizing candidates who can start in this location by set date as expected. We're keeping the situation under review and would adjust our position should the restrictive measures be removed later on.

Minimum qualifications:
Bachelor's degree in Computer Science or related technical field, or equivalent practical experience.
3 years of industry experience in software development, data engineering, business intelligence, data science, or related field with experience in manipulating, processing, and extracting value from datasets.
Preferred qualifications:
Master's degree in Computer Science, or related field.
Understanding of Big Data technologies and solutions (Spark, Hadoop, Hive, MapReduce) and multiple scripting and languages (YAML, Python).
Understanding of Google Cloud Platform (GCP) technologies in the big data and data warehousing space (BigQuery, Cloud Data Fusion, Dataproc, Dataflow, Data Catalog).
Demonstrable track record of dealing well with ambiguity, prioritizing needs, and delivering results in a dynamic environment.
Excellent verbal and written communication skills with the ability to effectively advocate technical solutions to research scientists, engineering teams and business audiences.
About the job


At Google, we work at lightning speed. So when things get in the way of progress, the Business Systems Integration team steps in to remove those roadblocks. The team identifies time-consuming internal processes and then builds solutions that are reliable and scalable enough to work within the size and scope of the company. You listen to and translate Googler needs into high-level technical specifications, design and develop recommended systems and consult with Google executives to ensure smooth implementation. Whether battling large system processes or leveraging our homegrown suite of Google products for Googlers themselves, you help Googlers work faster and more efficiently.

Data Engineers understand internal processes and what it takes to run Google at speed with its ever growing scale. As a Data Engineer, you'll focus on solving problems and creating value for Googlers by building solutions that are reliable and scalable to work with the size and scope of the company.

You will play a major role in developing, deploying, and supporting Google’s internal business applications. You will be tasked with creating custom-built software on google stack, and you will be part of teams that implement vendor sourced enterprise software, configuring that software, customizing it, and integrating with other internal systems.

Behind everything our users see online is the architecture built by the Technical Infrastructure team to keep it running. From developing and maintaining our data centers to building the next generation of Google platforms, we make Google's product portfolio possible. We're proud to be our engineers' engineers and love voiding warranties by taking things apart so we can rebuild them. We keep our networks up and running, ensuring our users have the best and fastest experience possible.
Responsibilities
Design, build and deploy internal applications to support our technology life cycle, collaboration and spaces, service delivery management, data and business intelligence among others.
Work closely with analysts and business process owners to translate business requirements into technical solutions.
Build internal solutions, with custom front ends (web, mobile) and backend services that automate business processes.
Maintain highest levels of development practices including: technical design, solution development, systems configuration, test documentation/execution, issue identification and resolution, writing clean, modular and self-sustaining code, with repeatable quality and predictability.

Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing this form.",Data Engineer,Bengaluru,4.4,$10+ billion (USD),"INR  15,53,774",Information Technology,10000+ Employees,Company - Public
"NVIDIA Corporation
4.5",-1,1993,-1,Computer Hardware & Software,"NVIDIA is looking for an elite software engineer to join its Product Security Tools team. The position will be part of a dynamic crew that develops and maintains database and automation infrastructure services provided by NVIDIA Product Software Security. NVIDIA is crafting a vision of incredible user experiences in the mobile, embedded and automotive spaces by combining our leading-edge GPU and Tegra development efforts into creative boundaries pushing and genre-defining products. The Security Tools team helps make this vision possible by providing critical infrastructure, automation and support to our global development team.

We need an engineer with a background in infrastructure and software tool and solutions development on various platforms. You will collaborate with multiple product teams to understand their needs and come up with solutions. You should thrive on being in the critical path supporting thousands of developers working for billion-dollar business lines as well as intimately understanding the values of responsiveness, thoroughness, and teamwork. You should constantly be championing and implementing efficiency improvements across their domain. You will be working directly with our security, legal, and global development teams. This is a critical role with the opportunity to make a big impact in an exciting company.

What you'll be doing:
Work as a Tools Development Engineer in the Security Tools team on various database, infrastructure and integration projects while interacting with other multi-site team members (India, China & USA) during product life cycle by participating in all stages of secure software development.
Design, develop, and maintain databases, web user interfaces, tools and reports for various stages of development.
Collaborate with Product Software Security, Legal, and Release Infrastructure teams to create & enhance ability to approve and track open sourced software throughout the SDLC.
Collaborate with Product Software Security, Development, and Release Infrastructure teams to automate onboarding to Static Analysis solutions.
Demonstrate craftsmanship and produce high-quality software that is unit tested, code reviewed and checked in regularly for continuous integration.
Provide technical leadership and use engineering best practices to initiate, plan, and execute large-scale, cross-functional, and company-wide projects Identify, leverage, and successfully evangelize opportunities to improve engineering productivity.
Constantly look for opportunities to improve the infrastructure and tools, including developing process guidelines, mentoring.
What we need to see:
5+ years of experience in software design and development to build infrastructure and automation frameworks at large scale.
Bachelor’s degree in Computer Science or equivalent qualification.
Extensive programming experience in Python.
Understanding of C++ or Java.
Development experience with HTML5, Web Services, and Docker.
Hands-on experience in backend development using SQL and relational databases with solid skill in writing SQL queries.
Knowledge of various Javascript libraries such as jQuery, KnockoutJS, Angular, etc.
Experience with source control systems such as Perforce and Git.
Proactive and able to work effectively across different functional teams.
Demonstrate excellent written and verbal communication capabilities.
Ways to stand out from the crowd:
Working knowledge of building automation infrastructure and test management tools is a huge plus.
Experience using and creating web services for data delivery with REST APIs.
Knowledge of continuous integration.
Experience in NoSQL Database (Elastic Search /MongoDB)
Familiarity with creating and maintaining robust web UIs.
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.","Senior Software Engineer, Product Security",Pune,4.5,$5 to $10 billion (USD),"INR  15,03,414",Information Technology,10000+ Employees,Company - Public
"Standard Chartered PLC
3.7",-1,-1,-1,Investment Banking & Asset Management,"Job: Technology
Primary Location: ASEAN & South Asia-India-Chennai
Schedule: Full-time
Employee Status: Permanent
Posting Date: 12/May/2020
Unposting Date: Ongoing

About Standard Chartered
We are a leading international bank focused on helping people and companies prosper across Asia, Africa and the Middle East.

To us, good performance is about much more than turning a profit. It's about showing how you embody our valued behaviours - do the right thing, better together and never settle - as well as our brand promise, Here for good.

We're committed to promoting equality in the workplace and creating an inclusive and flexible culture - one where everyone can realise their full potential and make a positive contribution to our organisation. This in turn helps us to provide better support to our broad client base.

Description:
Hands on working experience with Hadoop - Hive, Pig
Hands on experience of workflow/schedulers like NiFi/Ctrl M.
Hands on experience with Unix shell scripting
Experience with Apache Spark (required)
Good to have experience with Storm, Kafka, Spark Streaming
Familiarity with data loading tools like Flume, Sqoop.
Good understanding of Object oriented design, Design Patterns
Strong development experience is a must. Consistent track record for education and professional career
Motivation to learn innovative trade of programming, debugging and deploying
Self-starter, with excellent self-study skills and growth aspirations
Excellent written and verbal communication skills. Flexible attitude, perform under pressure.
Test driven development, a commitment to quality and a thorough approach to the work.
A good team player with ability to meet tight deadlines in a fast-paced environment

Requirement:
Hadoop data engineer
Apache Spark
Hive, NiFi, Kafka, Scala
Unix shell scripting

Responsibilities:
Develop complex data flow, data structures using Hadoop stack (NiFi, Hive, Scala, Unix Scripting)
Maintain, enhance, and implement bug fixes and support the product and data systems; proactively monitor events, investigate issues, analyse solutions, and drive problems through to resolution.
Collaborate with team on data solutions
Enforce good agile practices like story estimation and test-driven development.
Identify opportunities for continuous and automated deployment.
Introduce and follow good development practices, innovative frameworks, and technology solutions that help business move faster.
Drive continuous improvement and innovation.
Work with cross functional teams effectively

Experience: 4-7 years overall (overall), 2-3 years on Hadoop tools

Apply now to join the Bank for those with big career ambitions.

To view information on our benefits including our flexible working please visit our career pages.",Senior Developer-Big Data,Chennai,3.7,$10+ billion (USD),"INR  5,51,546",Finance,10000+ Employees,Company - Public
"Accion Labs Inc
3.8",-1,2011,-1,IT Services,"Sr Engineer (Big Data, Hadoop, Spark)

What you will do:

·Build large-scale batch and real-time data pipelines using the latest technologies to support production systems.

·Apply design-thinking and agile mindset in working with other engineers, and business stakeholders to continuously experiment, iterate and deliver on new initiatives.

·Leverage best practices in continuous integration and delivery.

·Help drive transformation by continuously looking for ways to automate existing processes, testing, & optimize data quality.

·Explore new capabilities and technologies to drive innovation.

What you need:

·5 to 7 years of experience building data products leveraging big data technologies (Hadoop, Spark, Kafka, ElasticSearch)

·Experience writing clean and concise code using Java / Scala / Python

·Experience with pipeline tools (Airflow / Luigi / Nifi)

·Experience with modern data warehousing tools (Presto / Snowflake / Redshift)

·Experience with public cloud environments

Nice to have:

·Knowledgeable about containers & orchestration (Docker, Kubernetes, Mesos)

·Knowledgeable about data modeling, data access & data storage techniques.

·Continuously learning the mindset and enjoy working on open-ended problems.","Sr Engineer (Big Data, Hadoop, Spark)",Bengaluru,3.8,$5 to $10 million (USD),"INR  13,31,456",Information Technology,1001 to 5000 Employees,Company - Private
"H & P Tax, H & P Consulting, Inc.
3.3",-1,-1,-1,-1,"Data Engineer

About Print Analytics

As part of the HP Inc. R&D Centre, Print Analytics Team work closely with Print GBU of HP Inc. across multiple domains. We deploy data products and analytics assets, provide data-driven actionable insights to influence business decisions.

Within Print Analytics, Supplies Analytics Team seeks to deploy data products through designing table structures and data flows, automating repetitive data tasks, building and maintaining data dictionaries, finding/resolving data anomalies and data errors. This would, in turn, result in delivering complex analysis and improving intelligence.

Business Environment

Supplies Analytics Team strategically partners with Big Data Business Transformation organization to deploy data products and deliver analytics to help them discover new opportunities and solve their business challenges in a data driven manner.

Role Description:
In order to deploy data products, we are looking for a high caliber and detail-oriented Data Engineer who can work in collaboration with cross-functional, cross-regional teams to establish and improve data pipelines. She / He will also be responsible for designing table structures and data flows, automating repetitive data tasks, building and maintaining data dictionaries, finding/resolving data anomalies and data errors. This would, in turn, enable downstream data analyses and AI/ML models.

The analyst would act as an informed team member who can help bridge technical data requirements.

Responsibilities:
Establishes secure and performant data architectures, enhancements, updates, and programming changes for portions and subsystems of data platform, repositories or models for structured/unstructured data.
Analyzes design and determines coding, programming, and integration activities required based on general objectives and knowledge of overall architecture of product or solution.
Writes and executes complete testing plans, protocols, and documentation for assigned portion of data system or component; identifies and debugs, and creates solutions for issues with code and integration into data system architecture.
Collaborates and communicates with project team regarding project progress and issue resolution.
Works with the data engineering team for all phases of larger and more-complex development projects and engages with external users on business and technical requirements.
Collaborates with peers, engineers, data scientists and project team.
Typically interacts with high-level Individual Contributors, Managers and Program Teams on a daily/weekly basis.
Defines and leads portions of project requirements for data exchanges and business requirements with externals and internal teams
Creates plans, data collection and analysis procedures and works with data insight visualization teams for assigned projects.
Collaborates with internal and external partners to perform experiments and validations in accordance with overall plan.
Collaborates with SMEs to develop procedures for collecting, recording, analyzing, and communicating data for review and feedback.

Education and Experience Required:
Bachelor's or Master's degree in Computer Science, Information Systems, Engineering or equivalent.
4-6 years’ experience in a data analyst or data engineering type role.

Knowledge & Skills:
Using data engineering tools, languages, frameworks to mine, cleanse and explore data.
Fluent in relational based systems and writing complex SQL.
Fluent in programming and automating repetitive tasks – preferably using VBA and Python
Fluent in complex, distributed and massively parallel systems.
Strong analytical and problem-solving skills with ability to represent complex algorithms in software.
Designing data systems/solutions to manage complex data.
Strong understanding of database technologies and management systems.
Strong understanding of cloud-based systems/services, including the AWS environment.
Database architecture testing methodology, including execution of test plans, debugging, and testing scripts and tools.
Ability to effectively communicate product architectures, design proposals and negotiate options at management levels.
Using scientific design and data collection methodologies, tools and analysis packages to collect, validate, and analyze research data.
Excellent written and verbal communication skills
Strong interpersonal skills and ability to work in a collaborative environment",Big Data Engineer,Bengaluru,3.3,Less than $1 million (USD),"INR  14,71,205",-1,1 to 50 Employees,Company - Private
"Apple Inc.
4.7",-1,1976,-1,Computer Hardware & Software,"Posted: Jul 20, 2020
Role Number:
200182043
We at Apple, new ideas have a way of becoming extraordinary products, services, and customer experiences very quickly. Bring passion and dedication to your job and there's no telling what you could accomplish.

Apple's Manufacturing Systems and Infrastructure (MSI) team is responsible for capturing, consolidating and supervising all manufacturing data for Apple’s products and modules worldwide within Apple’s Operations team. This data is stored and used during the entire product's lifecycle- from prototypes to mass production through warranty support for customers. Our environment develops product innovation, rapid iteration, and a liberating amount of autonomy. As an expert in developing software to manage large, multifaceted data sets, you'll be building platform for data ingestion, cleaning, transformation and evaluation to support a rapidly scaling organisation.
Key Qualifications
5+ years of professional experience with Big Data systems, pipelines and data processing
Hands on experience Big Data, data ingestion, data processing using Spark, Spark Streaming, Flink, HIVE, Kafka, Hadoop, HDFS, S3
Hands-on experience with design and development with NoSQL technologies Cassandra, HBase or similar scalable Key value-Stores and time series data stores like Druid, influx or similar
Understanding on various distributed file formats such as Apache AVRO, Apache Parquet and common methods in data transformation
Confirmed understanding of design and development of large scale, high efficiency and low latency applications is a plus
Understanding and experience with Micro Services is desired
Excellent problem solving and programming skills
Experience with containerization technologies like Kubernetes, Docker, Mesos, Marathon is desirable
Experience with CI/CD, debugging and monitoring applications and big data jobs is desirable
Description
- Develop solutions to answer sophisticated analytical and real-time operational questions
Help to design, architect and build the data platform using a variety of Big Data technologies
Design and develop applications involving data processing, hygiene, augmentation and transformation for distributed systems
Identify Data Validation rules and alerts based on data publishing specifications for data integrity and anomaly detection
Innovate by exploring, recommending, benchmarking, and implementing data centric platform technologies
Ensure operational and business metric health by supervising production decision points
Provide hardware architectural mentorship, estimate cluster capacity, and build roadmaps for Hadoop cluster deployment
Education & Experience
B.S., M.S., or PhD in Computer Science, Computer Engineering, or equivalent practical experience.",Big Data Engineer,Bengaluru,4.7,$10+ billion (USD),"INR  13,21,205",Information Technology,10000+ Employees,Company - Public
"Inteliment
4.2",-1,-1,-1,IT Services,"JOB DESCRIPTION

Data Engineer

Role : Data Engineer

Experience : 3 to 5 year

Education

Graduation / Post Graduation : Specialization in Computer Science, Software Engineering, Business Analytics etc.

Key Responsibilities

Designing and developing scalable ETL scripts from the business source systems and the development of ETL routines to populate databases from sources and to create aggregates.

Responsible for performing thorough testing and validation to support the accuracy of data transformations and data verification

Suggest & implement best practices for performance tuning while working on the larger datasets.

Development and implementation of scripts for database maintenance, monitoring, performance tuning, and so forth.

Ensure proper data governance and quality of the data.

Define standard data management principles and policies for retention and archival.

Troubleshoots data issues within the business and across the business and presents solutions to these issues

Analyse complex data elements and systems, data flow, dependencies, and relationships to contribute to conceptual physical and logical data models.

Provide real time knowledge transfer to the team on the Requirements / Design & Development.

Manage the infrastructure & deployment of the release artefacts by coordinating with respective teams.

Work in an agile environment with the defined sprints to deliver the assigned work in the stipulated timelines.

Excellence in implementing, configuring and using Big Data technologies, including Hadoop, Spark, Solr, Kafka, Hive and Impala, and Cloud Platform such as AWS and Azure

Adhere to software development best practices and coding standards in all work products and participate in the refinement of those practices and standards to improve quality and productivity.

Design & development of an event-processing pipeline that can handle millions of events.

Required Skills

Minimum 3 years of hands on experience in data management & engineering environment.

Design & development of a data-processing pipeline that can handle millions of rows.

Design an innovative methodology to extract information from data.

Strong knowledge of core software technologies and fundamentals – specifically for large-scale distributed systems – and building highly available services.

Ability to go across the full product development lifecycle – from design and development, to testing and deployment, to running large-scale, highly-available services in production..

Preferred having fundamental knowledge of scheduling, synchronization, IPC and memory management.

Familiarity with code versioning tools such as Git, SVN and Mercurial.

Understanding of Agile & Scrum development methodology.

Must to have the Knowledge of AWS Services.

Desired Skills

Demonstrated clear and thorough logical and analytical thinking, as well as problem solving skills

Self-directed, ability to work independently and research innovative solutions to business problems

Must be flexible to travel onsite if required.

Effective interpersonal communication across various levels of the organization.

Ability to interpret, evaluate and communicate detailed information in a manner that is appropriate to the audience.

Ability to conduct root cause analysis and performance tuning for complex business processes and functionality.

Ability to interact with IT and business users across the organization to resolve issues and provide solutions in a timely manner.

Should be proactive & transparent in the in the deliverables & critical thinker while designing the solutions.

Team oriented and enjoys working in a collaborative development environment.

Tools & Technologies

Languages: Java, JavaScript, Software Development, C++, Linux, Software Engineering, MySQL, SQL, XML, C, Agile Methodologies, Python, C#, jQuery, HTML, PHP, Web Services, CSS, Eclipse

Code Management: Git, SVN

Operating System: Mac, Linux, Windows

Databases Technologies: SQL & NoSQL Databases (Postgres, MongoDB, AWS S3, Redis) etc.

Cloud: AWS

If you have got it all.. What are you waiting for?",Data Engineer,Pune,4.2,$50 to $100 million (USD),"INR  27,831/mo",Information Technology,51 to 200 Employees,Company - Private
"JPMorgan Chase & Co.
3.9",-1,1799,-1,Investment Banking & Asset Management,"As a member of our Software Engineering Group, we look first and foremost for people who are passionate around solving business problems through innovation and engineering practices. You'll be required to apply your depth of knowledge and expertise to all aspects of the software development lifecycle, as well as partner continuously with your many stakeholders on a daily basis to stay focused on common goals. We embrace a culture of experimentation and constantly strive for improvement and learning. You'll work in a collaborative, trusting, thought-provoking environment-one that encourages diversity of thought and creative solutions that are in the best interests of our customers globally.

We are looking for a Software Engineer with experience in data warehousing and big data (Hadoop-based data platforms) domain. The role will be responsible for development of a data warehouse migration to a next generation enterprise data hub (EDH). The project will include the migration of reports (Cognos), analytical tools (QlikSense, Qlik View) and applications onto the new environment.
Senior developer/mentor to an existing team, designing solutions, best practices, defining standards and innovative solutions
Senior Dev Ops / TDD Journey inc CICD, Scanning, Code, Performance testing and Test coverage
Development using Agile methodologies (Jira / Scrum) delivered on time and in budget
Transforming existing ETL logic into Hadoop Platform in accordance with Architecture diagrams, performing required analysis & create Jiras
Innovate new ways of managing, transforming and validating data
Embed a culture of quality assurance best practices to the team
Data Quality Management - Establish and enforce guidelines to ensure consistency, quality and completeness of data
Learning cutting edge technologies and applications to greenfield projects
Support on legacy jobs which is primarily developed using unix shell scripts and oracle
This role requires a wide variety of strengths and capabilities, including:
Minimum 4+ Experience in a Big Data technology (Hadoop and Spark Architecture, Spark SQL, HIVE, SQOOP, Impala, HBASE, Entitlements etc., )
4+ years of Experience in Data warehouse or Database back ground
Strong Experience with UNIX shell scripting
Experience with Datawarehouses
Experience in Map Reduce
Experienced in writing SQL queries
Communicate complex issues in a crisp and concise fashion to multiple levels of management
Excellent interpersonal skills necessary to work effectively with colleagues at various levels of the organization and across multiple locations
Experience in the below would be advantageous:
Experience in cloud implementation
Experience in implementing complex ETL transformations in Hadoop platform
Experience with Python
Experience performing data analytics on Hadoop-based platforms
Experience in implementing distributed and scalable algorithms (Hadoop, Spark)
Experience in DevOps",Software Engineering - Big Data,Bengaluru,3.9,$10+ billion (USD),"INR  16,32,068",Finance,10000+ Employees,Company - Public
"TTEC
3.4",-1,1982,-1,Staffing & Outsourcing,"When everything's connected, how we connect is everything… and we'd like to connect with you too! We are looking for you to help us deliver exceptional customer experiences as a Data Engineer.

As a technologist, we know you’re in high demand. And we know it’s important you find the right fit for your future. Have ideas you want to contribute? We’re listening. Looking for exposure to different clients, different technologies? It’s what we do. Want to make an impact on the future? We’re innovating every day. Teamwork key? You'll have the opportunity to work on global projects with a knowledge-thirsty, international team. Join our inclusive Corporate IT team and you’ll help create meaningful employee experiences that drive memorable customer experiences.

What you’ll be doing
The data engineer will be responsible for all aspects of execution and delivery for new and ongoing client implementations of the analytic platform. This will require working with clients and coordinating with internal business consulting, analyst, data science, and technology teams.
Technical and functional analysis of customer implementation and integration requirements
Documentation of implementation requirements and expected effort
Configuration and setup of the analytics platform
Configuration of data loaders and development and configuration of workflow processes and customizations to the platform
Participate in testing
Proactive identification of internal and external dependencies, highlighting issues, scope changes, and progress against project plan
Communication of project status/issues to clients and internal management
Partner with various internal teams
Provide technical support to assist clients and partners during and post implementation.

What you'll bring to the role
Bachelor’s Degree in Computer Science, Information Systems or related with 2-3 years of relevant experience
In-depth familiarity with Big data technology and its application.
Proficiency with the Azure or AWS ecosystem
Experience with Big Data ETL
Understanding of complex data flows, identification of data processing bottlenecks and designing and implementing solutions.
Proficiency in .NET, C#, Python, Linux bash, Power Shell.
Experienced in consuming third-party REST APIs (JSON) and SDKs
A broad set of technical skills and knowledge across hardware, software, systems and solutions development and across more than one technical domain.
Experienced in U SQL, PostgreSQ, TSQL
Experience in professional services or technical consulting with enterprise software solutions, specifically enterprise software installation, configuration, customization, and testing.
Proven ability to balance and manage multiple, competing priorities.
Collaborative interpersonal skills and ability to work within cross-functional teams.
Self-starter who relies on experience and judgment to plan and accomplish goals in complex fast-paced environment to ensure quality of all data integration points.
Excellent customer service skills.
Creative problem-solving and analysis skills.
Ability to handle problem situations quickly, inventively, and resourcefully.
Project management skills including:
Ability to prioritize and manage tasks
Ability to plan, commit, and deliver to schedules
Ability to identify, escalate, and manage project issues
Willingness to work extended hours on an as-needed basic

#LI-IG1",Data Engineer,Hyderabad,3.4,$1 to $2 billion (USD),"INR  24,995/mo",Business Services,10000+ Employees,Company - Public
"Ganit Inc.
3.0",-1,-1,-1,-1,"Location: Bengaluru – India
Proven experience in working with Big Data ecosystems, AWS and Container services.
Design & development of features in existing products/new products using Java/Scala
ISupport large data volumes, design data processing pipelines and accommodate flexible provisioning of new sources.
Designing and developing ETL pipelines across multiple platforms and tools including Vertica, Hadoop, Hive and AWS Data Services
Good understanding of data warehouse schema design and granularity of the data
write to careers@ganitinc.com",Big Data Engineer,Bengaluru,3.0,Less than $1 million (USD),INR 388K - INR 423K,-1,51 to 200 Employees,Company - Private
"Jumio Inc
3.9",-1,2010,-1,Enterprise Software & Network Solutions,"Role Title: Infrastructure Engineer

Reporting to: IT Manager

Location: Jaipur/Udaipur

Role Purpose:

To accelerate and serve the business. Enable Jumio to grow and improve by executing on Infrastructure, Network, Security, Operating System, and Software projects.

This role is a combination of data center engineer, troubleshooter, sys admin, Server Administrator, project contributor and when needed project lead, system maintenance, future planning and capacity, migrations and moves. All this, in providing world class IT services in a timely, efficient and professional manner.

Role Description:
Design, build and configure applications to meet business process and application requirements.
Document Work - quality documentation is an integral part of the role. TheInfrastructure Engineer documents system configurations, updates to the system and any new procedures or policies created. Documentation is used by colleagues to understand changes to the system. Work is recorded in JIRA and documentation of workflows, solution methods, SOPs, procedures and site infrastructure is kept in Confluence.
Project Management - the Infrastructure Engineer often collaborates with IT support staff and coordinates with vendors and internal users when implementing new projects. Projects are listed as ‘Epics’ in JIRA. Responsible for setting up meetings, presenting information, ordering supplies and equipment and ensuring projects are delivered on time. Tracking and communicating progress effectively.
Additional Ad Hoc Duties - as assigned, based on business need.
Role Value: The Infrastructure Engineer is integral to the effectiveness of the business. IT serves the business through ensuring peak productivity and performance of the local teams and actively contributing to global projects. Jumio’s IT function is committed accomplishing The Mission: (1) Accelerate the Business and (2) Secure the Business.

Key Responsibilities :

Resource will work with diverse technologies to build tools and automation to eliminate manual operations and craft repeatable processes, thereby maintaining a highly available server infrastructure for deploying instances, applying operating system updates, managing configuration changes, and system tuning Resource will be responsible for the day-to-day operational support for AWS or Datacenter based hosts and their services, monitoring, troubleshooting and resolving problems, to improve the platform

Technical Experience:
Experience with enterprise scale Windows server administration Expertise in managing Active Directory, ADFS, Distributed File System DFS, Domain Controllers DHCP, Group policy management.
Advanced PowerShell scripting Experience High availability technologies such as failover clustering and load balancing Admin system health monitoring and optimizing performance Exceptional problem-solving skills.
Proactively maintain and develop all Linux infrastructure technology to maintain a 24x7x365 uptime service CentOS installation, upgradation, package deployment, SCD implementation, debugging, troubleshooting, vulnerability observation closure and day to day OS admin activities.
Previous working experience as a Linux Administrator for minimum 4 years.
In depth knowledge of Linux: RedHat, CentOS, Debian, etc.
Manage, coordinate, and implement software upgrades, patches, hot fixes on servers, workstations, and network hardware.
Proactively monitoring system performance and capacity planning.
Hands on experience with MySQL and/or MariaDB.
Familiarity with VMWare and SAN management and concepts.
Knowledge of Shell, Perl, and/or Python scripting will be considered an advantage
Solid knowledge of protocols such as DNS, HTTP, LDAP, SMTP and SNMP
Provide input on ways to improve the stability, security, efficiency, and scalability of the environment
Additional Linux certifications (RHCT, RHCE and LPIC) will be considered an advantage
Strong problem solving and communication skill
NOTE: Working experience in following is must
Linux and Windows OS patching
Kernal Patching
OS upgrade
Troubleshooting
Server Hardening
IP Change Management
LVM (Logical Volume Manager)
Network Bonding
Vulnerability Assessment and Penetration Testing Closure
User/Role Management
Deployment of certificate
Log Management

Professional Attributes:

Take part in 24x7 Shift Rotations including Morning, Evening and Night Shifts Work out of hours evenings, nights, weekends, bank holidays at reasonable request to perform production changes Travel to other locations when required Able to WFH occasionally Be flexible in the work a platform

Key Characteristics and Attitudes
Mature Big picture and the detail
Self-starting and managing Collaborative and collegiate
Urgency and ownership Humble and helpful
Resilient and tenacious Positive attitude and proactive
Measures that Matter

Keeping projects on time and within budget are key metrics for the Infrastructure Engineer role. Other measures for individual and team performance are taken from varied sources, including but not limited to, JIRA.

The Team

Infrastructure engineers work as part of Infrastructure IT team (currently 5 globally) and report to the IT Manager.

Helping teammates in both day to day and project work happens daily.

The function is committed to becoming a world-class team and moves fast, with intensity and accuracy.

Integrity and team cohesion are valued by all team members who subscribe to the belief that Team > Teamwork > Self.

The team is globally distributed. There will be Team Meetings and Project Meetings that are early in the morning and late at night due to the global distribution of colleagues.

Progression

In a rapidly expanding business opportunities for progression are frequent and unlimited. With new sites, growing teams and a license to explore the latest technologies there is no practical limit on progression.

@Work

Jumio has sites in India, London, Palo Alto, Vienna

Company

Jumio is the future for online and mobile ID verification. We are the largest and fastest growing company in the ID verification space. With a global footprint, we’re expanding the team to meet strong client demand across a range of industries including Financial Services, Travel, Sharing Economy, Fintech, Gaming, and others.

Equal Opportunities

Jumio is a collaboration of people with different ideas, strengths, interests and cultures. We welcome applications and colleagues from a wide range of backgrounds and statuses.",Infrastructure Engineer,Chhota Udaipur,3.9,$100 to $500 million (USD),INR 21K - INR 28K,Information Technology,201 to 500 Employees,Company - Private
"Cisco Systems, Inc.
4.3",-1,1984,-1,Computer Hardware & Software,"Platform Engineer

Who You Are
A Senior Platform Engineer with day-to-day responsibility of working with Business CX Applications teams, Program managers and Leads to design, build and deliver platform capabilities. To do this the Engineer should have required technical skills, deep knowledge of the required area and ability to learn and adapt to the needs of program. With this in mind, an understanding of the right skills, process understanding, and mindset is vital.
We are seeking high energy and experienced applicants who possess the following skills and experience:
8+ years of experience in working in complex environment building platform capabilities on cloud for deploying customer facing applications
The individual should possess the Platform thinking as he/she would be part of the Platform development team that will be leveraged by CX applications.
Ability to work well in a fast-paced, dynamic environment.
Critical thinking and big picture understanding of how platform should be built and maintained to enable applications scaling, security and resiliency
Good Interpersonal Communication Skills and you are willing to work with remote team
Flexible and adaptable: Ability to switch gears in various high-stress situations and apply themselves to quickly learning new technologies and adopting new methodologies.
Accountability: You are a results-oriented teammate who leads by example, holds self-accountable for performance, takes complete ownership, and champions all aspects of customer and project initiatives.
Efficient and Creative: You should be able to think creatively to find the optimal solution to problems.
Always should be ready to learn new technology, tools and process
What you’ll do:
Excellent verbal and written communication skills to a variety of technical and non-technical audiences is a requirement.
Excellent organizational skills.
Experience working in a team environment with a documented software development process; within an Agile methodology like Scrum.
Ambitious and hardworking individual who enjoys hard problems
Should have the thirst for learning new technology
Technically able to get things done while providing guidance to other team members
Desired Skills
Mandatory
Hands on experience on Java/j2ee, Spring, Python, Docker, Kubernetes, Microservices
Hands on experience on Distributed technologies below
Apache Spark
MapReduce principles
Kafka (MSK)
Data base and Caching
MySql
Redis
ElasticSearch / MongoDB
Experience with AWS technologies
API GW
Route53 DNS
ACM
Aurora
ElasticCache
EKS
Hands on experience building REST APIs, Swagger Specs
Good understanding of Client Server Architectures, Cloud Native technologies
Good understanding and experience in enabling Muti-Region and Multi-AZ architectures
Hands on experience on GIT, CI/CD tooling
Angular, JavaScript, HTML, Node JS
Hands on experience in writing Unit test cases, Integration test cases
Linux shell scripting
Proven experience in Agile environments
Good to have
Python, GoLang, gRPC
BigData technologies (Hadoop, Yarn, AWS EMR)
Serverless technologies like AWS Lambda, Fargate, Stepfunctions
AWS Cloudwatch, AppDynamics/Jaegar
Who you'll work with:
The CX Platforms, part of the CX Engineering seeks a highly motivated Senior Platform Engineer to join some of the industry's brightest minds in developing the platform for all of the CX Applications that are being built and planned. You will work with the CX Applications and CX Operations team to make sure we build a platform that is resilient, can scale and is secure. This is a critical role that requires us to work with various other organizations outside of CX.
Why Cisco
#WeAreCisco, where each person is unrivaled, but we bring our talents to work as a team and make a difference. Here’s how we do it.
We embrace digital, and help our customers implement change in their digital businesses. Some may think we’re “old” (30 years strong!) and only about hardware, but we’re also a software company. And a security company. A blockchain company. An AI/Machine Learning company. We even invented an intuitive network that adapts, predicts, learns and protects. No other company can do what we do – you can’t put us in a box!
But “Digital Transformation” is an empty buzz phrase without a culture that allows for innovation, creativity, and yes, even failure (if you learn from it.)
Day to day, we focus on the give and take. We give our best, we give our egos a break and we give of ourselves (because giving back is built into our DNA.) We take accountability, we take ambitious steps, and we take difference to heart. Because without diversity of thought and a commitment to equality for all, there is no moving forward.
So, you have colorful hair? Don’t care. Tattoos? Show off your ink. Like polka dots? That’s cool.","Software Engineer - Java/J2ee, Python, Node JS, MEAN Stack, SAAS, Cloud Native, AWS, Microservices, Docker, Kubernetes (9+ Years)",Bengaluru,4.3,$10+ billion (USD),"INR  12,66,484",Information Technology,10000+ Employees,Company - Private
"Roljobs Technology Services Pvt. Ltd.
3.8",-1,2002,-1,Staffing & Outsourcing,"Our client is a leading US based Product Development company in Bangalore/Gurgaon and is looking for Passionate Big Data Developers to join their team. We are seeking experts who are dynamic, self-motivated and out-of-the-box thinkers,

We would love to hear from you if:
You have minimum 4 to 8 years of experience in Java or its frameworks.
You have good hands-on experience on Big Data technologies such Hadoop, Spark, Hive, Kafka etc.
Have the ability to multitask and work in a fast-paced, collaborative team environment.
Are skilled in developing and maintaining an engineering task plan for an engineering team.
Education should be B. Tech. / M. Tech./ M.Sc/ MCA in Computer Science or Related fields.
We embrace diversity and equal opportunity in a serious way. We are committed to building a team that represents a variety of backgrounds, perspectives, and skills. The more inclusive we are, the better our work will be.",Big Data Engineer,Bengaluru,3.8,Unknown / Non-Applicable,"INR  18,388/mo",Business Services,51 to 200 Employees,Company - Private
"Analytics Vidhya
4.2",-1,-1,-1,-1,"Who can fill in the shoes?

Should be well-versed with the following tools and technologies of Big Data:

HDFS storage and compression formats
Concepts of MapReduce class in Java
Hive and Pig Latin
Sqoop and Flume
Oozie Interface for workflow management
NoSql – HBase, Cassandra, MongoDB
Spark – core, SQL, ML
Spark Streaming and it’s integration with Kafka

Good to know but not mandatory skills
Java programming experience
Experience with Complex Event Processing in near real time
Exposure to Flink
Some knowledge of Big Data Administration and service
YARN and Mesos
Zookeeper
Some knowledge of ML in Big Data context
What is the role?

Being a startup, the role would evolve over time. But, here are a few things you can expect:

Will be responsible for creating codes for training purpose using these tools under the guidance of a Senior Data Scientist
Will also work sometimes on the training content.

Where is the role based?

We would love to have you in our office in Gurgaon.

If the role excites you, drop an email to hiring@analyticsvidhya.com with your CV, mentioning “Why do you think you are the perfect fit for this role”.",Big Data Engineer,Gurgaon,4.2,Less than $1 million (USD),"INR  20,943/mo",-1,1 to 50 Employees,Company - Private
"Han Digital Solution (P) Ltd.
3.6",-1,-1,-1,Staffing & Outsourcing,"Job Description :


Minimum qualifications :
Build a big data system based on product requirements.
Should have strong knowledge of the Hadoop ecosystem such as Spark/HDFS/MapReduce/ Storm/Hive.
This will involve data extraction, data modelling, transformation, integration with other data streaming solutions.
Deep knowledge on SQL, additional exposure on other NOSQL databases like Mongodb would be an added advantage.
Experience in data migration from older systems of varied types to a big data architecture
Planning system & storage requirements for the big data platform.
Should have worked on Apache Spark SQL, Spark Streaming.
Must have worked with Python programming and integration with Apache Spark.
Preferred qualifications:
Candidates need to be proficient in Apache Spark (PySpark) with at least 1+ Years of work on live projects.
A Bachelor degree in B.E/B,Tech/BCA is must.
Strong working knowledge of SQL and NoSQL data stores such as Mongodb.
Strong programming knowledge in Python is preferred.
Key Job Attributes :


Big Data

Educational Qualifications :


B.E/B,Tech

Key Skills :


Python
Spark
HDFS
Mapreduce
Hive
hadoop
Nosql
MangoDB

Contact Details :


Email Id : anbu@handigital.com",Big Data Engineer,New Delhi,3.6,Unknown / Non-Applicable,"INR  3,89,631",Business Services,51 to 200 Employees,Company - Private
"Tavat Consultancy LLP
5.0",-1,2018,-1,Staffing & Outsourcing,"Company: Talent Corner Hr Services Private Limited
Experience: 4 to 5
location: Pune, Delhi
Ref: 24431682

Summary: Job Description :
Job Description

Selecting &integrating &Big Data tools &frameworks required to provide requested capabilities
Implementing ETL process{if importing data from existing data sources is….

Source URL: https://www.wisdomjobs.com",Big Data Engineer,India,5.0,Unknown / Non-Applicable,INR 893K - INR 980K,Business Services,1 to 50 Employees,Company - Private
"Sequretek – IT Security and Data Management
4.0",-1,2013,-1,Enterprise Software & Network Solutions,"About us & Vision

Sequretek is an Indian MNC focused on Information Security and Information Management space. The company is backed by industry veterans who have come together with a vision to build India’s leading Information Security company.
Sequretek’s customers have appreciated its solution offerings, and within a short span the company has acquired marquee clientele in Financial, Pharmaceutical, IT/ITES, and Retail and Logistics sectors.
Sequretek probably is the one of the very few companies that offers a blend of its own core threat intelligence products along with both on-premise and cloud solutions. Our end point detection, protection, and response technology – EDPR is the industry’s only product that replaces up to six different endpoint technologies for our customers.
Our vision is to establish and sustain Sequretek as a Global Leader in terms of the ‘Security’ of Enterprise-level Information-Assets through the consistent delivery of world-class products and solutions that leverage state-of-the-art technologies relevant to the contemporary digital economy.

Why Sequretek?

You will be part of an award winning ""Security Product Company of the Year – 2019” announced by Data Security Council of India (A NASSCOM Initiative).
You will be part of a highly visible agile team working on product development from scratch that directly affects the company’s success. Build time critical and data driven applications. Everyday different challenges to solve. Day to day job will put you in place to evaluate the technology length and breadth. It will also enable you to get an exposure to work alongside thought leaders & domain experts.

Education & Experience

Education:The candidate must have any of the below:
BE/B.Tech in Computer Science
M.C.A
Related field from reputed institute.
Experience:
Minimum 2 - 4 years of experience across design and architect large scale
real time data processing application in distributed environment.

Key Responsibilities

Skills:
Strong experience in Big data, Hadoop, HBase, Hive, Spark, Yarn, Reports
Analytics etc.
Strong experience in building highly scalable, high performance data
solutions in distributed/clustered environment.
Strong experience in Python/Scala, JAVA/J2EE, Linux/Unix/Windows.
Strong experience in designing data models for large enterprise application
which can handle millions of data in real time.
Strong experience in Search engine, Elasticsearch, Logstash, and Kibana.
Strong experience with Relational database and NoSQL Databases.
Strong experience experience in data management/warehouse strategies to
handle very large volume of data in real time.
Knowledge of messaging framework, Workflow, and Rule engine are added
advantage
Good to have experience in Machine Learning.
Must be proactive and flexible and have the ability to work under pressure
and possess good follow-through skills.
Must possess excellent written and verbal communication and a quick learner.
Responsibilities:
Design and architect application to process the real time large volume data
in a distributed environment.
End to end responsibility of designing and developing high performance
scalable data solution.
Extract complex reports out the data accumulated across different data
sources.
Apply Data warehouse strategies to manage the data and server to serve the
real time requests and reporting requirement.
Continuously improve the product’s performance, scalability and
manageability.
Build the required infrastructure for entire big data ecosystem which serve
requests in real time and fulfill complex report requirements.",Big Data Engineer,Bengaluru,4.0,Unknown / Non-Applicable,INR 10K - INR 14K,Information Technology,201 to 500 Employees,Company - Private
"Fidelity Management and Research LLC
4.0",-1,1946,-1,Investment Banking & Asset Management,"Job Description:


Job Title LEAD Aws Big data engineer

The Purpose of This Role

We are looking for a Lead- Big data Engineer with experience in aws based data lake and Data services. The role involves continuous collaboration with dev teams, driving best practices for operational management of cloud ready data services. Following Fidelity best practices / methodologies, thorough understanding of the technology roadmap, advancement to design / development process and providing innovative solutions at a very fast pace.

The Value You Deliver
Supporting and coordinating highly available data lake platform.
Leading best practice management for delivering secure and highly available data lake.
Supporting application teams with new data integration pipeline
Person will be responsible for designing and developing data lake platform
Data lake would support huge varied data sets with SLAs to load and process the data at a very fast pace. Ability to managenew data load and process pipelines.
Person will be using monitoring and dashboards for data pipelines both on premise and in the cloud.
The Skills that are Key to this role

Technical / Behavioral
Knowledge of operationalizing any big data solutions involving S3,EMR, EC2 and IAM
Performance tuning experience with different types of data stores both structured and unstructured.
Knowledge of programming languages/tools any two: Java, Python, Hadoop, Spark, SQL, Hive, Shell Scripts.
knowledge of logging, telemetry and data security on aws / azure
Experience with snowflake will be an added advantage
Good to have Knowledge ETL and ELT experience with both structured and unstructured data stores.
Understanding of data modeling and Continuous Integration (e.g. Jenkins, GIT, Concourse) tools
The Skills that are Good To Have for this role
Good to have AWS certification with RDS knowledge good to have.
Understanding of data validations, data cataloging, data lineage will be a plus.
Knowledge of indexing with Solar / elastic search and indexing approaches for data will be a plus.
Experience with analytics work bench. And data virtualization technologies like apache drill will be a plus
Experience with interacting and working closely with Product Owners, DevOps, and Software Engineering teams
Knowledge of big data design patterns is desirable.
Familiarity and experience working with agile methodologies: SCRUM
How Your Work Impacts the Organization

CTG is building a data lake to support business functions to run their analytics load.

The Expertise Were Looking For
7+ years of IT experience
Graduate
AWS certification
Location : Bangalore - Manyata/EGL / Chennai

Shift timings: 11:00 am - 8:00pm

Certifications:
Category:
Information Technology",Principal Software Engineering - Big data,Bengaluru,4.0,$10+ billion (USD),"INR  8,64,147",Finance,10000+ Employees,Company - Private
"BDB Limited
3.0",-1,1993,-1,-1,"Description
BizViz provides a 360 degree view of a business's data, serving any vertical and meeting the demanding needs of all business executives. With a 50+ strong team building the BizViz platform over several years, it is targeted at creating technological solutions that will give our customers the edge they need to succeed.

We strongly believe that our success lies in the success of our customers. We aim to build applications the way they envisioned, keeping each business' unique ideas and requirements in mind. We offer businesses a better alternative to using standard cookie-cutter ERP templates.

Job Summary
As a Big Data Engineer, you will be a member of a small, agile team of data engineers responsible for developing an innovative big data platform as a service for enterprises that need to manage mission critical data and diverse application stakeholders at scale. The platform manages data ingestion, warehousing, and governance, and enables developers to quickly create complex queries. The platform provides automatic scaling and elasticity to enable efficient use of resources, and provides services such as security, logging, and data provenance so that third party developers can focus their energy on algorithms rather than administration. We're looking for engineers who want a technical challenge. Help us improve the platform for our current customers and develop new capabilities for our future customers.

Responsibilities
Senior Big Data Engineer is an integral part of the Data Science Innovation team that works closely with Internal/External customer in all phases of the development.
Work with key stakeholders and understand their needs to develop new or improve existing solutions around data and analytics.
Work in a cross-functional, matrix organization, at times under ambiguous circumstances.
Partner in development of scalable solutions using large datasets with other data scientists on the team
Research innovative data solutions to solve real market problems.
Conceptualize, analyze and develop actionable recommendations for strategic challenges facing the organization.
Manage data analysis to develop fact-based recommendations for innovation projects.
Mine Big Data and other unstructured data to tap untouched data sources and deliver insight into new and emerging solutions.
Work with cross-functional teams to develop ideas and execute business plans.
Remain current on new developments in data analytics, Big Data, predictive analytics, and technology.
Education, Experience, Skills and Abilities Required for Consideration as a Candidate:
BTech/MCA degree or higher.
Minimum 5 year experience.
Candidates must be proficient in -
Java, Scala, Python
Apache Spark, Hadoop, Hive, Spark SQL, Spark Streaming, Apache Kafka
Well versed in various Predictive Algorithms, Mllib
Cassandra, RDMS (MYSQL, MS SQL, etc.), NOSQL, Columnar Databases, Big table.
Demonstrate a deep understanding of search engine technology. Ability to configure, index and maintain enterprise scale search applications. Proficient in writing custom handlers for Elasticsearch/Solr.
Agile development including Scrum and other lean techniques.
Excellent problem solving skills with the ability to design algorithms, which may include data cleaning, data mining, data clustering and pattern recognition methodologies
Ability to work cross-functionally in a highly matrix driven organization, at times under ambiguous circumstances
Personal qualities desired: creativity, tenacity, curiosity, and passion for deep technical excellence.
Location
Bangalore & Hyderabad.",Big Data Engineer,Bengaluru,3.0,Less than $1 million (USD),"INR  14,71,205",-1,51 to 200 Employees,Company - Private
"Tech9
4.3",-1,2015,-1,Computer Hardware & Software,"Tech9 is a fast growing custom software development company. We work to represent an ideal in software development delivery. We strive to make each client love us by providing a skilled team, engaging design, solid architecture, and quality implementation. We tackle big challenges with enthusiasm and gusto.


Tech9 India is looking for junior and senior Data Engineers. This is a great opportunity to work with a company that has a primary focus of making our customers happy by delivering value, without all the burdensome policies and rules that have become typical for outsourced software development companies.

If you are looking for a change this is what we can promise you:
You will have challenging problems to solve
You will have flexibility and autonomy to solve problems and deliver solutions
We will provide a highly collaborative environment with skilled and super friendly teammates
We will fully support you in developing software the right way
We won't burden you with useless policies and procedures
We will provide you the tools you need to do your job right
If that sounds attractive please apply! We'd love to talk to you.

Job Overview


We are looking for savvy Data Engineers to join our growing team. This person will be responsible for expanding and optimizing data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder who enjoys optimizing data systems and building them from the ground up. This Data Engineer will support our software developers on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.

Responsibilities for Data Engineer
Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Python, SQL and AWS
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications for Data Engineer
Working knowledge of SQL
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment
Experience with relational SQL databases
Knowledge of Object Oriented Development
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience developing in Python required
Experience with data pipeline and workflow management tools like Azkaban, Luigi, Airflow, etc preferred
Experience with Flask is preferred
Powered by JazzHR",Data Engineer,Pune,4.3,Unknown / Non-Applicable,"INR 2,130K - INR 2,338K",Information Technology,51 to 200 Employees,Company - Private
"Expedia, Inc.
3.8",-1,1996,-1,Internet,"Job Description:
Expedia Group is looking for a big data developer to build a data platform, turning event streams into actionable intelligence in real-time. In this role, you will be key to defining vision, design, and implementation from the ground up. This is not just an opportunity to build something new and innovative, but also to be at the center of Expedia transformation to a data-driven, smart platform for travelers and suppliers.

Are you ready to accelerate a large and growing travel business by combining flexible APIs, micro-services, and distributed data processing pipelines, designing it all to scale quickly using cloud computing platforms? Do you have the experience and judgment to establish new design patterns and mentor other developers, so we create a foundation that can be relied on for years to come? We'd love to talk to you!

What you’ll do:
Design and build high scale, real-time and batch data processing pipelines.
Be a part of our development team and actively participate in all phases of the software development lifecycle, including requirements gathering, functional and technical design, development, testing and roll-out, and support.
Solve complex business problems by utilizing disciplined development methodology, producing scalable, flexible, efficient and supportable solutions using appropriate technologies

Who you are:
5+ years of experience developing solid code in java or python, with a BS in Computer Science or equivalent.
Software development experience in big data technologies Hadoop, Hive, Spark.
Software development experience in real-time distributed computing with Storm, Kafka.
Solid experience in object-oriented programming, especially Java or Scala, design patterns, etc.
Strong computer science fundamentals in data structures, algorithm design, problem-solving, and complexity analysis.
Familiarity with SQL-based databases (Oracle, MySQL, etc.)
Familiarity with distributed systems and computing at scale.

Why Join Us?
Expedia Group recognizes our success is dependent on the success of our people. We are the world's travel platform, made up of the most knowledgeable, passionate, and creative people in our business. Our brands recognize the power of travel to break down barriers and make people's lives better – that responsibility inspires us to be the place where exceptional people want to do their best work, and to provide them the tools to do so.
Whether you're applying to work in engineering or customer support, marketing or lodging supply, at Expedia Group we act as one team, working towards a common goal; to bring the world within reach. We relentlessly strive for better, but not at the cost of the customer. We act with humility and optimism, respecting ideas big and small. We value diversity and voices of all volumes. We are a global organization but keep our feet on the ground, so we can act fast and stay simple. Our teams also have the chance to give back on a local level and make a difference through our corporate social responsibility program, Expedia Cares.

If you have a hunger to make a difference with one of the most loved consumer brands in the world and to work in the dynamic travel industry, this is the job for you.

Our family of travel brands includes Brand Expedia®, Hotels.com®, Expedia® Partner Solutions, Egencia®, trivago®, HomeAway®, Orbitz®, Travelocity®, Wotif®, lastminute.com.au®, ebookers®, CheapTickets®, Hotwire®, Classic Vacations®, Expedia® Media Solutions, CarRentals.com™, Expedia Local Expert®, Expedia® CruiseShipCenters®, SilverRail Technologies, Inc., ALICE, and Traveldoo®.

Expedia is committed to creating an inclusive work environment with a diverse workforce. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. This employer participates in E-Verify. The employer will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS) with information from each new employee's I-9 to confirm work authorization

Expedia is committed to creating an inclusive work environment with a diverse workforce.All qualified applicants will receive consideration for employment without regard to race, religion, gender, sexual orientation, national origin, disability or age.",Software Development Engineer- II (Big Data),New Delhi,3.8,$5 to $10 billion (USD),"INR  17,85,710",Information Technology,10000+ Employees,Company - Public
"Dotsolved Systems
4.9",-1,-1,-1,Computer Hardware & Software,"Looking for Data Engineer ( 3 positions) for Chennai India

Currently Remote work allowed post Covid Onsite.

Skills required

Apache Kafka

Pyspark/Livestreaming

Apache Airflow

Redshift

Job Types: Full-time, Contract

Pay: ₹586,412.00 - ₹1,796,683.00 per year

Experience:
work: 4 years (Preferred)
total work: 10 years (Preferred)
Education:
Secondary(10th Pass) (Preferred)
Work Remotely:
Temporarily due to COVID-19",Big Data Engineer,Chennai,4.9,$1 to $5 million (USD),INR 906K - INR 981K,Information Technology,1 to 50 Employees,Company - Private
Ceresight,-1,-1,-1,-1,"Location:: Anywhere in India (remote)
Key skills:: Hadoop technologies, ANSI SQL, Python/R/Scala

Desired Candidate Profile::
We are looking for a Big Data Engineer who will work on the collecting, storing, processing, and analysis of huge datasets from multiple data sources. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them.

You must have::
A good understanding of distributed computing principles, management of Hadoop cluster, with all included services;
Proficiency with Hadoop v2.x:: MapReduce, HDFS, Ambari, Zookeeper;
Good knowledge of Big Data querying tools, such as, Hive, Impala, or Big SQL
Experience with Python, R, or Scala;
Experience with integration of data from multiple data sources;
Deep Knowledge of various ETL techniques and frameworks, such as Datastage;
Experience with various messaging systems, such as Kafka; Experience with Cloudera/MapR or Hortonworks
Good understanding of AWS/GCP/Azure
Familiarity with Big Data ML toolkits, such as Tensorflow, SparkML, or H2O
Reach out to us if you have 3+ years experience in selecting and integrating any Big Data tools and frameworks and implementing solid ETL process, monitoring performance, recommending appropriate infrastructure and making required changes, defining data collection, retention and archiving policies, and interested in training and building a team of data engineers
Education:: - Bachelors/ Masters / Phd
We think the knowledge acquired by earning a doctorate or master’s degree in Computer Science with AI as a specialization would be of great value in this position, but if you're smart and have the experience that backs up your abilities, for us, talent trumps degree every time
Company Profile: This is the right place for you, if want to work in
A Data Science and Big Data technology start-up.
A place where you would want to create value for yourself, and our customers
An environment that supports your personal growth
Group of the best in class professionals who are excited about the work they do
Contact::
Sangeetha: sangpraman@gmail.com, +919655998843",Big Data Engineer,India,-1,Less than $1 million (USD),"INR  14,71,205",-1,Unknown,Company - Private
"PayU
3.4",-1,-1,-1,Internet,"Role: Big Data Engineer

Company: PayU Payments Pvt Ltd

Location: Gurgaon/Bengaluru

About Company:

PayU, the fintech-arm of Naspers, is a leading financial services provider in global growth markets. We use our expertise and heritage in cross border and local payments to extend the services we offer to merchants and consumers. Our innovative technology, developed in-house as well as through investments and strategic partnerships, empowers billions of people and millions of merchants to buy and sell online, extending the reach of financial services.

Our local operations span 18 growth markets across Asia, Central and Eastern Europe, Latin America, the Middle East and Africa. Here we deliver fast, simple and efficient financial services technology that unlocks access to more than 2.3 billion consumers in our regions.

Regulated under the Reserve Bank of India, PayU India has advanced solutions to meet every digital payment need. The company has an in-depth understanding of the vast and intricate details of the Indian market and its payment landscape. The company brings convenience and trust through continuous innovation leveraging technology.

PayU India forays into two business verticals - payment offerings under PayU Payments Services Ltd. and alternate lending under PayU Finance. Headquartered in Sohna Road, Gurgaon, the company has a presence in Mumbai, Pune and Bangalore and has a total strength of 700+ employees. Anirban Mukherjee is the CEO for PayU India working with the global CEO Laurent Le Moal.

Under the aegis of PayU Payments Services Ltd., PayU provides payment gateway solutions to online businesses through its cutting-edge and award-winning technology. In India, PayU covers nearly 60% of the airline business and 90% of the entire e-commerce business and processes over INR 120,000 crores worth of digital payments annually (at current run rates). The company offers more than 70 local payment methods and serves more than 350,000 merchants including leading e-commerce businesses in India. The company also empowers SMBs, enabling them to accept mobile and online payments with minimum development effort.

With credit being the key business priority, PayU has also developed LazyPay, an alternate lending platform to offer credit solutions such as Small Ticket Credit (Buy Now, Pay Later), App based personal loans and Point of Sale Credit (Merchant EMI). Since its launch in 2017, LazyPay has gained significant traction and has disbursed 20mn+ loans to a customer base of a million user.

PayU is bullish on investment opportunities in India. The company has been an aggressive investor, committed to the evolution of fintech in the country. PayU has spent about $250 million over the past three years in Asia's third-largest economy and is further scouting for more lucrative investment and acquisition opportunities to fuel growth.

PayU's acquisitions in India include that of Wibmo (April 2019 worth $70 mn) and Citrus Payment Solutions (September 2016 for $130 mn). PayU has also invested in PaySense (July 2018) and ZestMoney (December 2016) in India.

Role and Background Information:
Gather and process raw data at scale.
Design and develop data applications using selected tools and frameworks as required and requested.
Read, extract, transform, stage and load data to selected tools and frameworks as required and requested.
Perform tasks such as writing scripts, web scraping, calling APIs, write SQL queries, etc.
Work closely with the engineering team to integrate your work into our production systems.
Process unstructured data into a form suitable for analysis.
Analyze processed data.
Support business decisions with ad hoc analysis as needed.
Monitoring data performance and modifying infrastructure as needed.
Define data retention policies.
What you'd need to bring to the table:
2 7 years of recent experience in data engineering.
Bachelor's Degree or more in Computer Science or a related field.
A solid track record of data management showing your flawless execution and attention to detail.
Strong knowledge of and experience with statistics.
Programming experience, ideally in Python, Spark, Kafka or Java, and a willingness to learn new programming languages to meet goals and objectives.
Experience in C, Perl, Javascript or other programming languages is a plus.
Knowledge of data cleaning, wrangling, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks.
Experience in MapReduce is a plus.
Deep knowledge of data mining, machine learning, natural language processing, or information retrieval.
Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources.
Experience with machine learning toolkits including, H2O, SparkML or Mahout
A willingness to explore new alternatives or options to solve data mining issues, and utilize a combination of industry best practices, data innovations and your experience to get the job done.
Experience in production support and troubleshooting.
You find satisfaction in a job well done and thrive on solving head-scratching problems


So what do we offer?
A Competitive salary, including benefits
Modern offices with individual working spaces
Exceptional projects
Awesome teams that love finding ways of making things better, faster, stronger
Interesting growth prospects
ESOPs (SARs)
Education-

Bachelor of Engineering (IIT/NIT/Bits is Preferred)",Big Data Engineer,Bengaluru,3.4,Unknown / Non-Applicable,"INR  11,41,818",Information Technology,1001 to 5000 Employees,Company - Private
"Bobble Stickers & Keyboard App
4.0",-1,2015,-1,Internet,"Required Educational Qualifications

Bachelors in Computer Science and/or Mathematics from Tier 1 institute.

Required Experience:
At least 4 (four) years as an analyst and 2 years as Big Data Engineer.

What you will be doing here -
Develop and optimize a big-data analysis framework for standard reporting requirements.
Reporting trends & impact as per the cadence.
Analyse data from multiple angles, looking for trends that highlight problems or opportunities.
Cater to non-standard ad-hoc queries.
Manage data requests from internal & external stakeholders (like investors and business partners).
Interface & provide relevant input to AI & research team.
Ensure that data pipeline is healthy & exhaustive in nature, just-in-time.
Make recommendations to adopt new business and product strategies.
Preempt threats and opportunities.
Skills Required -
Mastery in writing MySQL or any other SQL.
Knowledge of Scala/Python is a good add on.
Skilled with Apache Spark framework.
Mastery in analytical tools like Excel Sheets or similar.
Mastery in representing raw data into relevant and easily comprehensible information.
Ability to break down complex problems into simpler parts
Ability to ask solution oriented questions

What makes you a perfect fit:
Leadership Skills
Entrepreneurial Mindset
Developing People
Analytical Ability
Attention to Detail
Problem Solving
Critical Thinking
Planning & Organization
Strategic Thinking
Innovative Thinking
Interpersonal Skills
Effective Communication
Active Listening
Teamwork
Effective Presentation
Personality Attributes
Curiosity
Discipline
Loyalty
Accountability
Result orientation
Proactive
Send your resume to hr@bobble.ai",Sr. Data Engineer,Bengaluru,4.0,Unknown / Non-Applicable,INR 517K - INR 721K,Information Technology,1 to 50 Employees,Company - Private
"Wavelabs Technologies Private Limited
4.4",-1,2015,-1,IT Services,"Wavelabs is an AI-First, new-age Technology company for the Digital, Cognitive & Industry 4.0 Era. We help you rethink and reinvent, while adapting to the inevitable change technology evolutions bring.

We leverage cutting-edge technology to become an enabler of fundamental innovation and disruption. We drive business outcomes with Data and Data Analytics to go beyond the existing platforms for insights and become agile on execution. We intend on leveraging the advances in artificial intelligence (AI) and machine learning (ML) to build solutions that are loved by all.

This is who we are. We are a community looking to deliver true flagship experiences. We focus on the things that matter most to us – design and customer experience. Most importantly, we’re not just doing it for ourselves. We are here to share the best technology and build beautiful products hand-in-hand with you.

Requirements
Strong Big Data background with working knowledge of data pipe lines, Snow Flake.

Experience in Data lake Architecture.

Experience in Data warehousing, data modeling, and SQL .

Experience in working on public cloud (AWS, Azure or GCP)

Experience with sourcing and modeling data from application APIs.",Big Data Engineer,Hyderabad,4.4,Unknown / Non-Applicable,INR 512K - INR 546K,Information Technology,51 to 200 Employees,Company - Private
DataKare Solutions LLC,-1,-1,-1,-1,"Job Brief

DataKare Solutions is a data analytics company that helps customer to adapt to new approaches and technologies to manage, govern, secure and apply advance analytics and data mining on enterprise data to improve operations.

We use big data technologies and latest cloud computing trends to fulfill software initiatives, enabling our customers to transform their industries.

We are looking for a full-time mid-level big data engineer with 1 to 3 years’ experience in the big data technologies. The ideal candidate should have strong programming skills either in Java or Scala.

your responsibility includes working on big data applications building data pipeline, APIs using Apache Spark Scala, HIVE, NO-SQL Hadoop environment.

Skills Required:
1-3 years’ experience in Scala, Java or Python programming languages.

Experience working with Hadoop, HDFS, Hive, SPARK, Unix scripting or any open source data ingestion tools.

Candidate should be comfortable with coding and passionate about learning new technologies and implement it all along.

Candidate with good academic background and completed training in big data technologies such as Apache Spark (Scala, Java or Python), Hive, Kafka, HBASE can also be considered for this position.

Database experience with at least one RDBMS(SQL, ORACLE)

Job Type: Full Time
Job Location: Hyderabad India",Big Data Engineer,India,-1,Less than $1 million (USD),INR 353K - INR 379K,-1,1 to 50 Employees,Company - Private
"ConnectIQ Labs Inc.
1.8",-1,2016,-1,Internet,"Miles is looking to expand its data science and data engineering team in INDIA!

Here's a quick checklist:
You live in India
Want to work for a fast-growing Silicon Valley Startup
You are passionate about solving challenging problems
You are looking to put your stamp on the product

What you'll need:
At least 4+ years experience
Confident in designing, building, managing, and owning a growing and complex data pipeline. Your experience should reflect success with something similar.
Strong understanding of relational DB such as MySQL/PostgreSQL, document-based storage systems such as MongoDB or CouchDB, key-value stores (Memcached, Redis), Column-oriented stores (HBase, Cassandra), graph-oriented stores; their distinct advantages, disadvantages, and trade-offs
Experience with building stream-processing systems, using solutions such as Kinesis, Kafka, Storm or Spark-Streaming.
Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala
Proficiency with Snowflake, Hadoop v2, MapReduce, HDFS
Knowledge of various ETL techniques and frameworks
Good understanding of Lambda Architecture
Develop and implement interactive analytic reports and dashboard using various tools such as Tableau, Looker, Chart.io
You should have worked on developing large-scale deployable software projects
Work closely with business and data science team for the deliverables.
Enjoy equity and influence in a fast-growing and dynamic pre-Series A company

Education
Master's (preferred) or Bachelor's (required) in CS/ML/AI or relevant computational/engineering discipline",Data Engineer,Bengaluru,1.8,Unknown / Non-Applicable,"INR  7,86,296",Information Technology,1 to 50 Employees,Company - Private
"Apple Inc.
4.7",-1,1976,-1,Computer Hardware & Software,"Posted: Apr 22, 2020
Role Number:
200166344
Imagine what you could do here. At Apple, new ideas have a way of becoming great products, services, and customer experiences very quickly. Bring passion and dedication to your job and there's no telling what you could accomplish.

We’re looking for a Big Data and Cloud Engineer who is hardworking and motivated to make an impact in creating a robust and scalable data platform. This is a hands-on Engineering role in the Operations Business Analytics team, which provides a unique opportunity to create innovations and ground-breaking change using cloud and big data technologies. You will be able to incubate and experiment newer technologies and platforms to demonstrate value of the data and enable data consumptions at scale with agility.

For this role we seek strong engineering skills and communication, as well as a belief that data driven processes lead to great products. You will need to have a passion for quality and an ability to understand complex systems.
Key Qualifications
Highly technical and analytical with 10 or more years of data engineering, analytics systems development and deployment experience
Strong verbal and written communications skills are a must, as well as the ability to work effectively across internal and external organizations and virtual teams.
Ability to think understand sophisticated business requirements and render them as prototype systems with quick turnaround time.
Knowledge of foundation infrastructure requirements such as Networking, Storage, and Hardware Optimization with Hands-on experience with any cloud Infrastructure (AWS, Azure, Google Cloud)
Track record of implementing Cloud based services in a variety of business such as large enterprises and start-ups.
Deep understanding of Data transformations , underlying technologies and understanding of related concepts (such as data cataloging and curation, etc.)
Demonstrated industry leadership in the fields of Data Warehousing, Data Science and Data processing.
Description
As Big Data & Cloud Engineer, you will work on a small team to develop large scale data pipelines and analytical solutions using Big Data technologies.
Understand current data landscape and develop architectural models that will operate at large scale and high performance, and advise on how to run these architectural models on on-Prem or Private Cloud infrastructure.
Experience in high level programming languages such as Java, Scala, or Python.
Proficiency with databases Teradata, MySQL, Postgres and SQL is required.
Proficiency in data processing using technologies like Spark Streaming, Spark SQL, or Map/Reduce.
Expertise in Hadoop related technologies such as HDFS, Azkaban, Oozie, Impala, Hive, and Pig.
Expertise in developing big data pipelines using technologies like Kafka, Flume, or Storm.
Experience with large scale data warehousing, mining or analytic systems.
Work with analysts to capture requirements and translate them into data engineering tasks.
Aptitude to independently learn new technologies.
Extract best-practice knowledge, reference architectures, and patterns from these engagements for sharing with the worldwide architect community and engineering teams.
Education & Experience
Bachelors or Masters Degree in Computer Science or Mathematics background preferred",Big Data & Cloud Engineer,Hyderabad,4.7,$10+ billion (USD),"INR  13,21,205",Information Technology,10000+ Employees,Company - Public
"Pitney Bowes Inc.
3.4",-1,1920,-1,Computer Hardware & Software,"The Job

• Ethically hack applications to find security vulnerabilities.
• Create security solutions in support of product line development.
• Supply expert knowledge in the fields of security, privacy, and identity and the related issues, systems, processes, products, and services and provide security advisory to product development teams.
• Implement proof of concepts of new security technologies that will be incorporated into products.
• Execute the design and implementation of products to ensure appropriate and effective security and data protection controls are included.
• Drive secure software development techniques for applications.
• Execute static and dynamic analysis of software and work with teams on remediation of findings.

Qualifications & Skills required
• B.Tech/B.E. OR PG M.S. / M.Tech
• 2-4 years of Security Experience Required.
• CISSP, CSSLP certifications required, (or is currently using related knowledge in existing projects, ability and existing personal goal to complete certification within six months of hire this is a must)
• Offensive security experience is MUST; Bug Bounty Hunting experience is preferred.
• Ethical Hacker- should be able to use Automated DAST tools against Web Application, Web Services and Mobile Applications (Android (MUST), iOS)
• Work in SAST tools such as Checkmarx CxSAST, CxOSA, and HP Fortify etc. a big plus
• Demonstrated ability to work in a fast-paced multi-tasking Agile environment
• Demonstrated technical leadership and teamwork skills encompassing internal and external resources.
• Demonstrated excellence in English communication skills with multiple stakeholders: clients, management, employees, and vendors.
• Experience with AWS security features is added advantage
• Experience with Docker security reviews is added advantage
• Experience with Automation using Jenkins/ TeamCity for integration of security tests in CI/ CD
• Network security experience with proxy services and SSL/TLS is preferred.
• Experience with Cloud based WAF & DDOS Solutions.

The Team

Our passionate and ambitious team delivers innovations that help clients navigate the complex and always evolving world of commerce: from helping, them use data to market to the best customers, to enabling the sending of parcels and packages efficiently, to securing payments through statements and invoices.

Our Global FDR Innovation team is dedicated to using the best-in-class tools, processes and modern architectures to create great experiences for our clients. In a rapidly changing world, we have a clear technical vision for our future that includes SaaS, APIs, Big Data, Advanced Analytics, Mobile and the Internet of Things. We are also focused on creating great client experiences, utilizing a Design Thinking platform and approach.

Helping clients achieve their greatest commerce potential are Pitney Bowes' 14,000+ passionate employees around the world, our relentless pursuit of innovation with over 2,300 active patents, and our focus on clients, who are at the center of all that we do - from small businesses to 90% of the Fortune 500.

In everything, we do, we deliver accuracy and precision to drive meaningful impact.

Pitney Bowes is an Equal Employment Opportunity/Affirmative Action Employer that values diversity and inclusiveness in the workplace.",ADVISORY SOFTWARE ENGINEER,Pune,3.4,$2 to $5 billion (USD),"INR  26,10,697",Information Technology,10000+ Employees,Company - Public
"Rockstar Games, Inc.
4.0",-1,1998,-1,Video Games,"At Rockstar Games, we create the games we would want to play ourselves.

A career at Rockstar is about being part of a team working on some of the most creatively rewarding, large-scale projects to be found in any entertainment medium. You would be welcomed to a friendly, inclusive environment where you can learn, and collaborate with some of the most talented people in the industry.

Rockstar India is on the lookout for talented Data Engineers who possess a passion for Game Analytics. This is a full-time permanent position based out of Rockstar's unique game development studio in Bangalore, India.

WHAT WE DO
The Rockstar Analytics team provides insights and actionable results to a wide variety of stakeholders across the organization in support of their decision making.
We work together with a number of departments to design and implement data and pipelines.
We collaborate as a global team to develop cutting-edge data pipelines, data products, data models, reports, analyses and machine learning applications.
RESPONSIBILITIES
Resolve operational issues as they occur to maintain the team's SLAs.
Implement and support big data tools and frameworks such as HDFS, Hive, and Impala.
Implement and support data models using Spark and Spark-ML.
Assist in the development of deployment automation and operational support strategies on Hadoop and Snowflake.
Deliver near-real time and non-near-real-time data and applications to a team of analysts and data scientists who create insights and analytics applications for our stakeholders.
Maintain and extend our CI/CD processes and documentation.
QUALIFICATIONS
5+ years of work experience with ETL, Data Modeling, and Business Intelligence Big Data Architectures.
5+ years of experience with the Hadoop ecosystem (Map Reduce, Spark, Spark-ML, Oozie, Hive, Impala, etc.) and big data ecosystems (Kafka, Cassandra, etc.).
Experience developing and managing data warehouses on a terabyte or petabyte scale.
Experience developing Machine Learning pipelines and data models.
Strong experience in massively parallel processing & columnar databases.
Experience with Python and shell scripting.
Experience working in a Linux environment.
Deep understanding of advanced data warehousing concepts and track record of applying these concepts on the job.
SKILLS
Expert in at least one SQL language such as T-SQL or PL/SQL.
Good communication skills.
Dynamic team player.
A passion for technology - we are looking for someone who is keen to leverage their existing skills and seek out new skills and solutions.
PLUSES


Please note that these are desirable skills and are not required to apply for the position.
Experience in real-time analytics applications.
Experience in Lambda architecture and On-Premise Clusters.
Experience with Java or Scala programming languages.
Experience with CI/CD.
Knowledge of RestAPI and Artifactories.
Knowledge of the video game industry.
HOW TO APPLY


Please apply with a CV and cover-letter demonstrating how you meet the skills above. If we would like to move forward with your application, a Rockstar recruiter will reach out to you to explain next steps and guide you through the process.

Rockstar is proud to be an equal opportunity employer, and we are committed to hiring, promoting, and compensating employees based on their qualifications and demonstrated ability to perform job responsibilities.

If you've got the right skills for the job, we want to hear from you. We encourage applications from all suitable candidates regardless of age, disability, gender identity, sexual orientation, religion, belief, or race.",Data Engineer,Bengaluru,4.0,$10 to $25 million (USD),"INR  5,60,617",Media,1001 to 5000 Employees,Subsidiary or Business Segment
"Zinnov Management Consulting
3.7",-1,2002,-1,Consulting,"About the Draup:
DRAUP is a stealth-mode start-up, incubated at Zinnov, and working on Big Data and Machine Learning. We are building an Enterprise Sales Enablement platform, which will enable huge multi-corporations to be able to sell better. We are a 10-month-old team creating a new product led by very experienced Serial Entrepreneurs with more than 12 years of experience in the sales industry with a good track record of creating and selling off a very successful start-up.

The Big Data Engineer at Draup is responsible for building scalable techniques and processes for data storage, transformation and analysis. The role includes decision-making and implementation of the optimal, generic, and reusable data-platforms. You will work with a very proficient, smart and experienced team of developers, researchers and co-founders directly for all application use cases.

Experience:
B.E / B.Tech / M.E / M.Tech / M.S in Computer Science or software engineering.
Experience of 2-6 Years working with Big Data technologies.
Open to embrace the challenge of dealing with terabytes and petabytes of data on a daily basis. If you can think out of the box have good code discipline, then you fit right in.
Responsibilities:
Develop, maintain, test and evaluate big data solutions within the organisation.
Build scalable architectures for data storage, transformation and analysis.
Design and develop solutions which are scalable, generic and reusable.
Build and execute data warehousing, mining and modelling activities using agile development techniques.
Leading big data projects successfully from scratch to production.
Creating a platform on top of stored data sources using a distributed processing environment like Spark for the users to perform any kind of ad-hoc queries with complete abstraction from the internal data points.
Solve problems in robust and creative ways.
Collaborate and work with Machine learning and harvesting teams.
Skills:
Proficient understanding of distributed computing principles.
Must have good programming experience in Python.
Proficiency in Apache Spark (PySpark) is a must.
Experience with integration of data from multiple data sources.
Experience in technologies like SQL and NoSQL data stores such as Mongodb.
Good working Knowledge of MapReduce, HDFS, Amazon S3.
Knowledge of Scala would be preferable.
Should be able to think in a functional-programming style.
Should have hands-on experience in tuning software for maximum performance.
Ability to communicate complex technical concepts to both technical and non-technical audiences
Takes ownership of all technical aspects of software development for assigned projects.
Benefits:
Expertise in big data infrastructure, distributed systems, data modelling, query processing and relational.
Involved in the design of big data solutions with Spark/HDFS/MapReduce/Storm/Hive.
Worked with different types of file-storage formats like Parquet, ORC, Avro, Sequence files, etc.
Strong knowledge of data structures and algorithms.
Understands how to apply technologies to solve big data problems and to develop innovative big data solutions.
Someone with entrepreneurial mind-set delivering quick and efficient solutions with good design and architectural patterns will be preferred.",Big Data Engineer,Bengaluru,3.7,$1 to $5 million (USD),"INR  7,41,882",Business Services,51 to 200 Employees,Company - Private
"Postdot Technologies, Inc
4.9",-1,2014,-1,Enterprise Software & Network Solutions,"requirements
You have at least 1 year of experience working and scaling with data pipelines and warehouses.
You have good understanding of at least 1 programming language.
You have worked on problem statements including batch-processing.
You know the basics and internal workings of at least one database/data warehouse/data lake and understood the workings in depth.
You have a knack to deep-dive into the use-cases of data, not just the technical aspects.

Bonus Requirements

Knowledge about AWS Redshift data warehouse
Understanding of AWS resources and serverless computing
Knowledge on Kubernetes (preferable Amazon EKS)
Exposure to any orchestration tool (preferably Airflow)
description

Postman leads the way in the API-first universe. Postmanâ€™s API Development Environment is used by 10+ million developers and more than 500,000 companies to access 250+ million APIs every month.

We are looking for a Data Engineer in data team to help us scale the existing infrastructure and in parallel work on next generation data tools including data scrapping, machine learning infrastructure and data validation systems.

Data team at Postman acts as a central function, catering to the needs of the overall organisation. More than half of the organisation is currently active on our data tools, hence data engineers and data analysts work together to cater to these forever increasing needs. We are a lean team which works autonomously by delegating and trusting every member to take things to completion.

Responsibilities

Be an amazing learner on a day-to-day basis.
Communicate effectively with data consumers to fine-tune data platform systems (existing or new).
Contribute to existing EL (extract & load) data pipelines while building new systems in parallel.
Own and deliver high performing systems (not just pipelines) and help the team scale them up, to endure ever increasing traffic.
Become a product owner (not just a system owner) over-time by understanding the end results of building systems.
benefits

We offer a competitive salary and excellent benefits. What you will also get to experience is a company that believes in autonomous small teams for maximum impact; that strives for organizational growth to align with that of the individual; that continuously and purposefully builds an inclusive culture where everyone is able to do and be the best version of themselves and where ideas are encouraged from anyone and everyone. We seek people who naturally demonstrate our values, who not only understand the challenge but can also solve this for the rest of the world. Be a part of something big.",Data Engineer,Bengaluru,4.9,Unknown / Non-Applicable,"INR  14,65,989",Information Technology,51 to 200 Employees,Company - Private
"Deutsche Bank AG
3.5",-1,1870,-1,Banks & Credit Unions,"Job Description:


Job Title: AVP – Big Data Scala

Location: Pune, India

The Engineer designs and develops application code, implements technical solutions, and configures applications in different environments in response to business problems. To meet the requirements of the business, the Engineer actively participates in the design and architecture of the application or its components, investigates and proposes appropriate technologies to be used, promotes re-usability of existing components and contributes to the creation of frameworks. Assists more junior members of the team and controls their work where applicable.

What we’ll offer you

As part of our flexible scheme, here are just some of the benefits that you’ll enjoy
Best in class leave policy
Gender neutral parental leaves
100% reimbursement under child care assistance benefit (gender neutral)
Flexible working arrangements
Sponsorship for Industry relevant certifications and education
Employee Assistance Program for you and your family members
Comprehensive Hospitalization Insurance for you and your dependents
Accident and Term life Insurance
Complementary Health screening for 35 yrs. and above
Your key responsibilities

Essential:
Hadoop Cloudera distribution – Manager, Navigator
Apache Spark – Streaming , structured streaming, and batch processing.
Big data SQL Engines – Hive & Impala
Scala – Core, Concurrency, Akka Actors
Apache Kafka – Publish , subscribe, partitioning, delivery semantics
HDFS – Access APIs, Operations, CLI
Oracle – SQL, PL/SQL

Nice to have:
Python 3 through the Anaconda distribution
Python Analytics libraries: Numpy, Pandas, Koalas, Arrow, Matplotlob etc
Apache Airflow, Tensorflow, Spark MLib, Scikit
Jupyter Notebooks/Hub
Apache Flink, Flume, Sqoop
Caching Tech – EhCache, Apache Ignite
Cassandra, HBase

How we’ll support you
Training and development to help you excel in your career
Flexible working to assist you balance your personal priorities
Coaching and support from experts in your team
A culture of continuous learning to aid progression
A range of flexible benefits that you can tailor to suit your needs
About us and our teams

Please visit our company website for further information:

https://www.db.com/company/company.htm

Our values define the working environment we strive to create – diverse, supportive and welcoming of different views. We embrace a culture reflecting a variety of perspectives, insights and backgrounds to drive innovation. We build talented and diverse teams to drive business results and encourage our people to develop to their full potential. Talk to us about flexible work arrangements and other initiatives we offer.

We promote good working relationships and encourage high standards of conduct and work performance. We welcome applications from talented people from all cultures, countries, races, genders, sexual orientations, disabilities, beliefs and generations and are committed to providing a working environment free from harassment, discrimination and retaliation.

Click here to find out more about our diversity and inclusion policy and initiatives.",Big Data Scala - Engineer,Pune,3.5,$10+ billion (USD),"INR  17,08,991",Finance,10000+ Employees,Company - Public
FitFyles LLP,-1,-1,-1,-1,"FiFyles is changing lives!

As a Lead Architect for Data, you’re eager to jump into a brand new learning experience. We need a Lead Architect for Data to have fun analyzing complex, massive data sets, building amazing tools and gaining insights from it all. If this sounds like you, please read on!

Who You Are

You entrepreneurial instincts recognize the opportunity of working at a startup-within-a-startup
You are extraordinarily well versed in the latest data architectural and modeling principles
You are likeable
You believe every experience, good or bad, is an opportunity to learn and do better.
You are convinced that the best use of experience is to leverage it to learn more
You are like a kid in a candy shop when it comes to new database technologies
You are respected and your opinions have enormous influence among your peers
Your enthusiasm easily persuades others
You believe that deadlines are sacrosanct
People come to you to solve their disagreements
You are a data-modeling god
You can navigate the maze of columnar and NoSQL databases as comfortably as your keyboard
You’re politically neutral within your organization

What You Want From Your Next Career Move

To change the world by managing a team of data researchers, engineers and analysts. To learn and grow. To help people engage with their health and well-being through data.

Why We Need You

FitFyles is using data to deliver the best health information to the users that need it. Find insights in data to help us save lives. At FitFyles, our success is based on our ability to dive deep into complex and data-intensive challenges to find scalable solutions to some of today's most pressing health care problems.

We need your unique skills to:
Build tools and products that will help us discover unexpected insights and understand data to drive better decisions
Help define and track, alongside our product and engineering teams, the metrics that will measure our success, from the utility of new features to our ultimate goal of measurably prolonging life expectancy
Pioneer new uses of data to create meaningful user growth and engagement
Grow and manage a team of top-tier data researchers, engineers, and analysts to help change the way people think about and relate to their health and health data

How You’re Changing Global Healthcare

Using data, you’ll connect people with trustworthy physicians,to bring concierge medicine and peace of mind to millions of people around the world.

What you’ve accomplished:
Masters Degree (PhD is a plus) in a quantitative discipline (applied mathematics, statistics, CS, or related field)
3+ years of experience in big data analysis- especially in building large scale, efficient and real-time solutions
Very strong logical problem solving abilities and a solid knowledge of the tools of the trade, including common and open data and analytical frameworks like- Spark, Spark SQL, Hadoop, SQL, R, etc.Working knowledge of scripting languages, especially Ruby on Rails, and Python.
A thorough understanding of A/B testing and experience implementing it at scale (preferred)
Startup or equivalent experience (preferred) and the drive to live the dream (required)",Data Engineer,New Delhi,-1,Less than $1 million (USD),"INR  14,71,205",-1,Unknown,Company - Private
"Twilio Inc.
4.0",-1,2008,-1,Internet,"Because you belong at Twilio.

The Who, What, Why and Where

Twilio is growing rapidly and seeking Data Engineers at multiple levels to be a key member of the Consumer Trust Team in Bangalore, India. You will be joining one of the first teams of engineers in our new Bangalore office, with an opportunity to help define our technical and team culture in India. You will also help us build solutions that prevent fraud and abuse, ensuring that Twilio is the leader in trusted communications. A successful candidate will be a self-starter, embody a growth mindset, collaborate effectively, can mentor junior engineers and operate highly resilient services.

Who?

Twilio is looking for a strong data engineer who lives the Twilio Magic and has a demonstrated track record of working with data, specifically; sourcing and integrating data from multiple disparate backend data sources, developing reporting infrastructures and applying a deep analytics background to assess business performance and deliver actionable insights to improve efficiency and increase productivity. You should also have::
BA/BS in Computer Science, Engineering or related field
Relevant work experience in a role requiring application of analytic skills to integrate data into operational planning/business planning
Knowledge and expertise with database modeling and data warehousing principles
Fluent in writing and optimizing SQL, data mining using SQL with demonstrated strength in writing complex, high-optimized queries across large data sets,
Familiar with AWS services, especially S3, Redshift, big data services and DevOps tools
Hands on experience with Lucene / SOLR / ElasticSearch, Kafka, Google Big Query
Proficiency in at least one scripting language, Python, R, or similar.
Advanced ability to draw insights from data and clearly communicate them (verbal/written) to the stakeholders and senior management as required
Demonstrated ability to manage and prioritize workload and roadmaps
Excellent problem solving, critical thinking, and communication skills.
Strong belief in automation over toil.
Nice to have:
Hands-on experience with Big Data technologies (e.g Hadoop, Hive, Spark) is a big plus
Extensive knowledge of BI and Visualization platforms i.e. Tableau and AWS Quicksight
Strong expertise in troubleshooting complex production issues.
What?

As a Data Engineer you will:
Design, develop, and maintain data pipelines, warehouses, and reporting systems to support Twilio's products, including fraud and abuse detection systems/ tools.
Design, develop, and maintain data pipelines, warehouses, and reporting systems to support Twilio's product engineering operational data: incidents, deployments, performance, utilization, defects, change failure rate, test data, infrastructure costs.
Build the data products that technical users will depend on for business intelligence and ad-hoc access.
Build scalable solutions and self-serve platforms that will provide data and KPIs to inform business decision making.
Identify, develop, manage, and execute analyses to uncover areas of opportunity and present written business recommendations that will help improve the controllership and help achieve the goals of the team.
Develop and maintain documentation relating to all assigned systems and projects
Develop high-trust relationships and processes with partner teams and stakeholders to identify and address insight requirements
Participate in workstreams planning process including inception, technical design, development, testing and delivery of BI solutions.
Be able to adapt to prioritizing multiple issues in a high-pressure environment.
Be able to understand complex architectures and be comfortable working with multiple teams.
Why?

Twilio has democratized communications channels like voice, text, chat, and video by virtualizing the world's telecommunications infrastructure through APIs that are simple enough for any developer to use, yet robust enough to power the world's most demanding applications.

The Consumer Trust Team is central to Twilio's continued growth. Our mission is to prevent consumer harm by offering products and services that protect our customers and help them authenticate their users. We also ensure that every call, email and message that is made using our service is wanted, safe and legal. To do this we need to continue to develop and evolve our products and services and ensure they are able to scale; driving Twilio to new heights of scale.

Twilio is a company that is empowering the world's developers with modern communication in order to build better applications. Twilio is truly unique; we are a company committed to your growth, your learning, your development, and your entire employee experience. We only win when our employees succeed and we're dedicated to helping you develop your strengths. We have a cultural foundation built on diversity, inclusion, and innovation and we want you and your ideas to thrive at Twilio.

Where?

This position will be located in our office in Bangalore, India. You will enjoy our office perks: catered meals, snacks, game room, ergonomic desks, massages, bi-weekly All Hands and more. What you will also get to experience is a company that believes in small teams for maximum impact; seeks well-rounded talent to ensure a full perspective on our customers' experience, understands that this is a marathon, not a sprint; that continuously and purposefully builds an inclusive culture where everyone is able to do and be the best version of themselves.

About Us

Millions of developers around the world have used Twilio to unlock the magic of communications to improve any human experience. Twilio has democratized communications channels like voice, text, chat, video and email by virtualizing the world's communications infrastructure through APIs that are simple enough for any developer to use, yet robust enough to power the world's most demanding applications. By making communications a part of every software developer's toolkit, Twilio is enabling innovators across every industry — from emerging leaders to the world's largest organizations — to reinvent how companies engage with their customers.",Data Engineer,Bengaluru,4.0,$1 to $2 billion (USD),"INR  14,71,205",Information Technology,1001 to 5000 Employees,Company - Public
"Qubole Inc.
4.4",-1,2011,-1,Enterprise Software & Network Solutions,"Qubole is a simple, open, and secure Data Lake Platform for machine learning, streaming, and ad-hoc analytics. Our platform provides end-to-end services that reduce the time and effort required to run Data pipelines, Streaming Analytics, and Machine Learning workloads on any cloud. No other platform offers the openness and data workload flexibility of Qubole while lowering cloud data lake costs by over 50 per cent. Qubole customers process nearly an exabyte of data every month. Qubole investors include Charles River, Institutional Venture Partners, Lightspeed, Norwest, Harmony and Singtel Innov8.

High quality and comprehensive customer support is one of the core strengths of the Qubole team. As a member of our Technical Support team - you will interact with users of Qubole Data Services (QDS) via Support Tickets, Chat, Email or Phone. Support issues range all the way from simple queries about the product, to questions around achieving specific technical objectives, to responding to and troubleshooting operational issues. Customers depend on Qubole to run their business - and our Technical Support team is the most important part of providing that assurance.
What you'll be doing
Provide resolutions and/ or Workaround to customer queries/ issue as appropriate
Ensure that Service Level Agreements are met for each assigned incident
Reproduce customer issues for diagnosis/ further analysis, passing acknowledged Service errors to Engineering team for fixing and QA
Complete Ownership: Track Support issues through to closure upon complete agreement with Customers at all times
Willing to assist Solutions (Professional Services) team and other teams within the Company when needed or as situation demands
Visit Customers when needed
Be involved in Knowledge Sharing (Knowledge Base Articles, Documentation, Forums, Blogs, etc...)
Exhibit continuous improvement on technical knowledge and problem resolution skills and strive for excellence
Required experiences and skills
3 - 8 years of experience in Technical/ Application Support
Bachelors degree in CS/Eng required, masters/ Ph.D. a plus
Excellent written and verbal communication skills
Ability to read and write SQL - and understanding of one of Relational Databases such as MySQL, Oracle, Postgres, SQL Server
Comfortable with Linux with ability to write small scripts in Bash/Python. Ability to grapple with log files and unix processes
Ability to learn complex new things quickly
Occasional involvement in support over weekends/ early/ late shifts (6AM-10PM window at worst - working remotely)
Be a team player with an ability to work under pressure with good time management skills
Having these will set you apart as a candidate
Knowledge of Java/Python
Knowledge of AWS and related concepts like EC2, S3, ELB, VPC etc.
Experience with Hadoop/Hive and/or related technologies like Cassandra etc.
Experience supporting multi-tiered applications
Qubole is hitting that growth inflection point where we need talented people to help us scale up. Our company culture is special, and we are looking for people to join us who want to continue building a great company while going after the big data activation market.
Check us out on Glassdoor and LinkedIn
Learn more about us here, here, and here

Culture at Qubole
Trust and Autonomy: We absolutely pride ourselves on the lack of bureaucracy at work, and believe in delegating power and responsibility, aggressively to our employees.
Transparency and Teamwork: Complete transparency in all our thoughts and actions is integral to our genetic character, and it helps us to stick together and function effectively as a team.
Who Thrives: If you are a self-starter and thrive on complexity and independence and truly understand and live the tenets of humility, hunger and honesty and you will love Qubole.

Qubole is an Equal Employment Opportunity employer that proudly pursues and hires a diverse workforce. Qubole does not make hiring or employment decisions on the basis of race, color, religion or religious belief, ethnic or national origin, nationality, sex, gender, gender-identity, sexual orientation, disability, age, military or veteran status, or any other basis protected by applicable local, state, or federal laws or prohibited by Company policy. Qubole also strives for a healthy and safe workplace and strictly prohibits harassment of any kind.",Big Data Support Engineer,Bengaluru,4.4,Unknown / Non-Applicable,"INR  52,632/mo",Information Technology,201 to 500 Employees,Company - Private
"Alphonso Inc
4.1",-1,2012,-1,TV Broadcast & Cable Networks,"Locations: Bangalore, New York

Alphonso data platform processes hundreds of millions of data-points about tv and adviewership data from the entire country. We plan to continue to invest in drawing deep insights from this vast pool of tv data. You will be responsible for developing scalable data models, machine learning algorithms to unlock new insights from TV data, innovate on targeting algorithms and data graphs to drive the business value further.

Requirements:
PhD in Computer Science or equivalent.
8+ years in handling high volume (hundreds of millions of records) data sets
Proficiency in one or more of Python, Java or JavaScript
Experience with machine learning algorithms and/or statistical modeling
Familiarity with Big data technologies like Hadoop, Map/Reduce, Spark, Hive etc. is a plus",Data Engineer,Bengaluru,4.1,Unknown / Non-Applicable,"INR  30,84,051",Media,51 to 200 Employees,Company - Private
"Sanofi
3.7",-1,1973,-1,Biotech & Pharmaceuticals,"· Mission statements

o Responsible for building the pipelines and tools in the DARWIN environment

o Produce data sets for specific analysis projects, dashboards and applications, and general reuse.

o Identify data engineering needs and implement optimal solutions.

o Provide subject matter expertise to other users who are working in the DARWIN environment.

People

o Maintain effectiveness relationships with the end stakeholders with an end objective to develop education and communication content as per requirement

o Interact effectively with healthcare professional on publications content

o Constantly assist other writers in developing knowledge and sharing expertise

Performance

o Create and maintain optimal data pipelines using Sanofi’s data platform and infrastructure

o Assemble large, complex data sets that meet the needs of data analysts and data scientists as per agreed timelines and quality

o Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.

o Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.

o Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.

o Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.

o Create data tools for analytics and data scientist team members that facilitate their discovery, access and use of data.

Process

o Build processes supporting data transformation, data structures, metadata, dependency and workload management.

o Successfully manipulate, process and extract value from large disconnected datasets.

o Bring Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.

Stakeholders

o Work with data and analytics experts to strive for greater functionality in our data systems.

o Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.

Knowledge, Skills & Competencies / Language

o 5+ years of experience in a Data Engineer role

o Experience supporting and working with cross-functional teams in a dynamic environment.

o Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.

o Strong project management and organizational skills.

o Strong analytic skills related to working with unstructured datasets.

o Excellent English language knowledge

Qualifications

o Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field

Requirements of the job

o Experience and knowledge PySpark programming

o Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.

o Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.

o Build processes supporting data transformation, data structures, metadata, dependency and workload management.

o A successful history of manipulating, processing and extracting value from large disconnected datasets.

o Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.

At Sanofi diversity and inclusion is foundational to how we operate and embedded in our Core Values. We recognize to truly tap into the richness diversity brings we must lead with inclusion and have a workplace where those differences can thrive and be leveraged to empower the lives of our colleagues, patients and customers. We respect and celebrate the diversity of our people, their backgrounds and experiences and provide equal opportunity for all.",Data Engineer,Hyderabad,3.7,$10+ billion (USD),"INR  4,05,386",Biotech & Pharmaceuticals,10000+ Employees,Company - Public
"MNR Solutions
4.6",-1,2007,-1,Staffing & Outsourcing,"Requirements:
â— Experience in one or more languages but not limited to: Java/C/C++/C#/ Python/ JavaScript /Go.
â— Interest and ability to learn other coding languages as needed.
â— Experience working within a Linux computing environment, and use of command line tools including knowledge of shell/Python scripting for automating common tasksÂ

Experience
â— Demonstrable experience designing technological solutions to complex data problems, developing & testing modular, reusable, efficient and scalable code to implement those solutions.
â— Knowledge of Databases - SQL/NoSQL Databases and Graph Databases
â— Knowledge of Algorithms and Data structures
00-2.00 Years",Big Data Engineer,New Delhi,4.6,Unknown / Non-Applicable,INR 22K - INR 24K,Business Services,51 to 200 Employees,Company - Private
"Sysvine Technologies
4.4",-1,2000,-1,IT Services,"Responsible for delivering high-value next-generation products to write high-quality, highly optimized/high-performance and maintainable code
The position is for a project for a multinational client of ours who builds platforms for retail analytics using big data
The project is long-term, probably 3-5 years

Desired Candidate Profile

Experience with Python or Scala in HDFS, Big Data Technologies
Apache Spark with Python or Scala, preferable with AZURE HDInsight
Hands on experience with RDBMS Design, preferable with Hbase, Hive, or Cosmos
Good knowledge of Data architecture, Big Data architecture, and ETL processes
Design and development of data ingestions, reporting, analytics and visualization

Education/Specific Knowledge

B.S. or M.S in Computer Science or relevant discipline
PG: ME/M.Tech – Computers or Any Specialization
Doctorate: Doctorate not required

Key Skills

Big Data, HDFS, Cosmos, Reporting, Analytics, HBase, Big Data Architect, Hive, Software Engineering",Senior Software Engineer – Big Data,Chennai,4.4,Unknown / Non-Applicable,"INR  7,53,091",Information Technology,51 to 200 Employees,Company - Private
"Atlassian
4.4",-1,2002,-1,Computer Hardware & Software,"Atlassian is continuing to hire with all interviewing and on-boarding done virtually due to COVID-19. Everyone new to the team, along with our current staff, will temporarily work from home until it is safe to return to our offices.


Are you passionate about data ecosystems and the ability to use data to drive actionable change? If so this role with our team at Atlassian is for you. As a Data Engineer on the Customers Success and Support team, you'll be driving our business to scale via building and improving our data infrastructures and data pipeline.

Some examples of what you will be doing are:

· Architect, build, launch, and manage data models to enable analytics
· Design, build, and manage data warehouse
· Design, build, improve, and manage data pipelines and ETL jobs
· Create ETL scripts via SQL/HiveQL/SparkSQL
· Automate data pipeline and reporting processes
· Build data expertise and own data quality for the awesome pipelines you build

Some of the tools/languages/systems you'll use are:
· Hive, Spark, Postgres, Presto
· Amazon EC2, EMR, S3
· Python
· Docker
· Tableau
· SourceTree and Bitbucket
· Linux Shell
You'll work together with other data engineers, analysts, project managers, and subject matter experts to deliver impactful outcomes to the organization. You'll participate in multiple concurrent high-visibility projects along with occasional ad-hoc questions from your internal customers.
We continually require modifications to the data pipeline for improvements in quality, speed, and features. In this role you'll focus on building out the future state of our data pipeline. You'll help design event collection infrastructure, build data models, and ETL processes to collect, extract, and clean the data for subsequent reporting and analysis. The target is making our data model more scalable, reliable, maintainable, and better integrated with other parts of our data ecosystem.
More about you
You've been in a data engineering role for 5+ years and have a BS degree in Engineering, Computer Science, or other technical discipline. You value high-quality work with attention to detail and have a track record of delivering both. You combine curiosity with critical thinking and good judgment, and like asking ""why"" to unravel a seemingly complex problem and get to the root cause. You know how to gather, document, and interpret business requirements.
You're a wizard with SQL; better than anyone else you know. You've got practical experience working with large structured or unstructured datasets. You enjoy thinking about improvements to the ways in which data is consumed and then figuring out how to make it happen via reporting platforms and visualizations. You have a nearly insatiable desire to learn new concepts and technologies and apply them to your work.
When you encounter a problem you come up with multiple solutions, weigh the tradeoffs and efforts, identify the best path forward, and exercise good judgment to drive ahead. You're comfortable interacting with people across all levels of an organization and can field questions during a presentation like a pro. You're self-driven and find ways to be impactful.
You're quick on your feet and take on challenges with ease. You can take an ambiguous assignment and derive valuable insight. You use multiple tools and methods to find solutions, and couple that with intuition and quick tests to prioritize how to unravel complicated problems.
More about our team
You'll be joining a growing analytics and project delivery team located in multiple regions across the globe. We challenge each other constantly to improve our work and ask hard questions. We're direct, focused, and demand excellence, but there's laughter in every meeting because we thoroughly enjoy the work we do and the impact it has. We're constantly growing, learning, adapting, and trying new things. BBQ, tacos, and coffee are a few of our favorite things.
More about Atlassian
Software is changing the world, and we’re at the center of it all. With more than 100,000 global customers (including 85 of the Fortune 100) and a highly disruptive business model, we’re advancing the art of team collaboration with products like JIRA, Confluence, Trello and Stride–and we’re just getting started. Motivated by honest values, an amazing culture, and consistent revenue growth, we’re out to unleash the potential of every team. From Amsterdam and Austin to Sydney and San Francisco, we’re looking for people who are powered by passion and eager to do the best work of their lives in a highly autonomous yet collaborative, no B.S. environment.
Atlassian. Powered by You.
But Wait, There’s More...
https://www.youtube.com/watch?v=k14BTBwQjJ0
More about our benefits

Whether you work in an office or a distributed team, Atlassian is highly collaborative and yes, fun! To support you at work (and play) we offer some fantastic perks: ample time off to relax and recharge, flexible working options, five paid volunteer days a year for your favourite cause, an annual allowance to support your learning & growth, unique ShipIt days, a company paid trip after five years and lots more.

More about Atlassian

Creating software that empowers everyone from small startups to the who’s who of tech is why we’re here. We build tools like Jira, Confluence, Bitbucket, and Trello to help teams across the world become more nimble, creative, and aligned—collaboration is the heart of every product we dream of at Atlassian. From Amsterdam and Austin, to Sydney and San Francisco, we’re looking for people who want to write the future and who believe that we can accomplish so much more together than apart. At Atlassian, we’re committed to an environment where everyone has the autonomy and freedom to thrive, as well as the support of like-minded colleagues who are motivated by a common goal to: Unleash the potential of every team.

Additional Information

We believe that the unique contributions of all Atlassians is the driver of our success. To make sure that our products and culture continue to incorporate everyone's perspectives and experience we never discriminate on the basis of race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status.

All your information will be kept confidential according to EEO guidelines.",Senior Data Engineer,Bengaluru,4.4,Unknown / Non-Applicable,"INR  12,18,484",Information Technology,1001 to 5000 Employees,Company - Public
"SS&C Technologies Holdings, Inc.
2.8",-1,1986,-1,Computer Hardware & Software,"Overall responsible and accountable for all Encounter activities at DSTWS India. This includes following activities

Minimum of 3 + years software development experience
1+ years’ experience on Cloudera technologies like Impala,Skala and Hadoop
1+ years working with Hadoop technology
Confidence in the ability to write highly concurrent programs in Java/Scala
knowledge on NOSQL databases like HBase
A demonstrated ability to learn new technologies quickly
Deep passion for building great software
Good written and oral skills
Familiarity with one scripting language like python
Experience in both Agile and traditional software development environments
Experience with at least one Functional Programming language
Possesses a deep understanding of distributed systems
Ability to build data models
Integrate with products and gather requirements to build data sources.
Expedite Analyst skills

Location:
Hyderabad Andhra Pradesh

DST is an equal opportunity employer and values a diverse and inclusive workplace. All qualified candidates will receive consideration for employment without regard to age, race, color, religion, genetic information, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law. For more information about Equal Opportunity in the Workplace please click here and here . DST is committed to working with and providing reasonable accommodation to job applicants with disabilities. If you are a qualified individual with a disability and need an accommodation or accessibility assistance to complete the online application, please contact us at PeopleCenter@dstsystems.com for assistance.","Senior Software Engineer, Big Data",Hyderabad,2.8,$2 to $5 billion (USD),"INR  6,03,507",Information Technology,10000+ Employees,Company - Public
"Alcon
3.6",-1,1945,-1,Biotech & Pharmaceuticals,"Position Title
Data Engineer

14-Jul-2020

Job ID
298921BR

Job Description
Support the planning, design, development and delivery of system solutions for a specific business or technology area. Capture new IT demand and ensure IT services provided to business are being delivered to requirements and that business user expectations and satisfaction levels are met.
Act as a point of contact for the business-specific business capability, for existing and new services to ensure that agreed services are being delivered to requirements and business user expectations and satisfaction levels are met. -Ensure service adherence with quality, compliance and security standards and delivery of associated corrective service actions; handle and resolve customer feedback. -Design, develop and/or deliver solutions that meet business requirements. Job Purpose Purpose of the role is to build is to design and build solutions to ingest data from variety of sources in to Data and Analytics platforms at Alcon. As part of the Analytics team, work closely with Business stakeholders to understand Analytics needs, create, support or enhance Analytics products that align with Alcon’s Data and Analytics strategy and standards. Essential Job Functions •As part of the Analytics sprint team before data engineering to ingest data from various sources into data lake solution on AWS Cloud as per the roadmap •Deliver business solution on Alcon’s analytics platform through end-to-end implementation that includes data security, governance, cataloging, preparation, automated testing, and data quality metrics. •Contribute towards building high performing platform/product DevOps agile teams •Automate, optimize, migrate and enhance existing solutions. •Perform data modeling, data analysis and providing insights using various tools.
-Completeness and accuracy of business demand capture -Project/solution delivery execution (quality, time, cost) -Process efficiency -IT service risks recorded / managed -Customer Satisfaction -Service/solution quality: timely and accurate reporting (i.e. Service performance and CSAT analyze) -Adherence to solution architecture and architecture roadmaps -Adherence to SLAs.
Applied Business Insights Digital & Technology Savvy Organizational Savvy Project Excellence Continuous Learning (Dyn. Knowledge Development) Interpersonal Savvy Being resilient.

Minimum requirements
Collaborating across boundaries Cross Cultural Experience Organization Scope; Scale and Complexity Ambiguity Accountability English IT Data Analysis IT Systems Design Programming / Software development IT Systems Integration IT Testing IT Customer Service Support IT Service Level Management.

•Education: Bachelor’s degree or equivalent years of applicable experience •Experience: 3+ years of applicable experience in Data Warehousing and BI Solutions. 3+ years of experience in writing code in spark engine using python, scala or java Language The ability to fluently read, write, understand and communicate in English. Specific Job Posting Content •Good understanding of traditional and latest data and analytics platforms architecture models. •3+ years experience in data engineering which includes data ingestion, preparation, curation, provisioning, automated testing, and quality checks. •2+ years of Big Data, and Analytics Technologies – hands-on experience is a must Big Data cloud platforms, Data Lakes, and Data Warehouses Familiarity with Visualization and Reporting Tools like Tableau, Salesforce Einstein Analytics and QlikSense 3+ years experience in languages to ingest data is a must – SQL, Java, Python, & Scala 

Job Type
Full Time

Country
India

Work Location
Bangalore

Functional Area
Information Technology

Division
ALCON

Business Unit
NON-NVS AL INFORMATION TECHNOLOGY

Employment Type
Regular

Company/Legal Entity
Alcon Ind

Shift Work
No",Data Engineer,Bengaluru,3.6,$5 to $10 billion (USD),"INR  11,63,646",Biotech & Pharmaceuticals,10000+ Employees,Company - Public
"Noon Academy
4.1",-1,2014,-1,K-12 Education,"Noon is making education fun by social learning and collecting a lot of data points, which help to make product and customer experience better. Data comes from a variety of sources, internal and external, and in different formats. The data that we collect in a heterogeneous and distributed storage (data lake).

For our users (employee, customer) to be productive, data needs to be accessible uniformly. While understanding the nature of the data they deal with, our users should not be held responsible for its quality. The collected data might end up containing duplicates, being outdated, and generally being corrupted in several ways; one of the jobs of a data engineer is to prevent these issues and to minimize their occurrence.

We massage data, put its pieces together to provide a different perspective, elaborate it, store it, manipulate it, and optimize its access. We are also in the process of building pipelines to make the data flow from one sector of our architecture to another. We write efficient software, maintain our clusters, optimize and advise on queries, and continuously re-evaluate our process to make it faster and more accurate. To do this, we keep the discussion open, share our findings, and hold each other accountable.

We are in the middle of a substantial transformative process. We are piecing together a data science architecture that needs to sustain tons of gigabytes of data in a dependable and distributed way so that the data is at the fingertips of its users. If you haven’t scared yet? Then maybe you might consider joining forces with us.

Requirements
Excellent knowledge of SQL and Python
Experience with cloud platforms, in particular, Amazon Web Services
Preferred knowledge of Java / Scala
Good understanding of SQL and NoSQL databases, Star schema (data modeling, data warehousing)
Excellent Experience of Python, Java or Scala
Experience with big data: Hadoop, Spark, Kafka, Flink (HDFS, HBase, Hive)
Knowledge of algorithms and data structures
Deep understanding of the distributed systems
Experience with data visualization tools like Tableau or ElasticSearch will be a big plus
What can you expect
Work with a team who believes in STUDENT FIRST, you will have an opportunity to build the first (BE ORIGINAL) Open Social Learning Platform that can impact millions of students across the globe
A working environment where you can look at things differently and challenge and offer solutions; which also offers you a freedom to commit ‘n’ number of first time mistakes - NEVER DEPRIVE A LEARNER
Complete ownership and responsibility for the success of your autonomous team across all scope of works - OWN IT and BE BOLD
An opportunity to lead bunch of Young, Smart, Driven and Dynamic engineers who are committed to go BEYOND SELF to solve business challenges
Work closely with the leadership team who live by the value of BE BETTER EVERYDAY to help growing an amazing organisation",Data Engineer,Bengaluru,4.1,Unknown / Non-Applicable,"INR 965K - INR 1,036K",Education,51 to 200 Employees,Company - Private
"Amantya Technologies, Inc",-1,-1,-1,-1,"Big data, Spark, Hadoop, Hive, Scala, ETL, SQL, AWS

Experience: 6 - 10 yrs
Location: Gurgaon, India

Job Title : Big data Developer
Skills : Big data, Spark, Hadoop, Hive, Scala, ETL, SQL
Notice period : max 30 days

Qualifications :
– Bachelor’s Degree or Master’s Degree with 6 to 10+ years of experience in Computer Science or related field.
– Strong experience in ETL and expertise in SQL.
– Hands on expertise in Big Data Ecosystem with experience in Hadoop, Hive, Spark, Strome, Cassandra, NoSQL DB’s, Spark, Scala.
– Experience in distributed programming, scripting and writing SQL
– Cloud Development experience like AWS
– Experience in building scalable/highly available distributed systems in production.
– Understanding of stream processing with knowledge on Kafka.
– Knowledge of Software Engineering best practices with experience in implementing CI/CD, Log aggregation/Monitoring/alerting for a production system.
– Good level of Expertise in production support related activities (issue identification, resolution)
– Excellent communication skills
– A self-motivated learner and mentor with strong customer focus and obsession with quality

Responsibility :
– Develop high performance and scalable solutions that extract, transform, and load big data.
– Design, build, test and deploy cutting edge solutions at scale, impacting millions of customers worldwide drive value from data at Client Scale
– Experience performing root cause analysis on data and processes to answer specific business questions and identify opportunities for improvement.
– Experience building and optimizing ‘big data’ data pipelines, architectures and data sets involving petabyte and terabyte of data.
– Interact with Client engineering teams across geographies to leverage expertise and contribute to the tech community.
– Engage with the engineering team to provide seamless production support.",Big Data Engineer,Gurgaon,-1,Less than $1 million (USD),INR 863K - INR 943K,-1,1 to 50 Employees,Company - Private
"Touchnote
5.0",-1,2008,-1,Internet,"Who we are:

We are reinventing personal communication for the digital era.

TouchNote is a creative platform that lets people send custom-made cards, straight from their phone to those they love anywhere in the world. Our easy to use app has helped people nurture their most meaningful relationships over 15 million times and was awarded the Good Web Guide’s App of the Year 2018.

We are a team of passionate and creative individuals trying to make a difference. We’re proud to offer a highly collaborative, solution-focused environment that celebrates diversity and has been listed in Deloitte’s Technology Fast 50 and The Financial Times Future 100 UK.

The Role:

We are looking for a savvy Data Engineer to join our growing data and analytics team.

Our ideal candidate is an experienced data pipeline builder and data wrangler who enjoys both optimising existing data systems and building new capabilities from the ground up. They will be working alongside our Data Lead and Data Analysts to formalise and expand our data collection and delivery architecture - ensuring information is accurate, timely, consistent, meaningful, and enabling our teams to build analytics simply on top of robust fundamentals.

They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimising or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

What you'll do:
Create, maintain and monitor an efficient data pipeline architecture, including working with scheduled jobs, replication, monitoring and partitioning.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Handle changes in application logic and develop supporting data transformations.
Ensure data integrity and consistency throughout, and enable teams to easily verify this in end analysis.
Own database and data flow documentation.
Develop and maintain a large event stream to enable real-time customer analytics.
Work with a wide variety of business stakeholders to assist with data-related technical issues and support their data infrastructure needs.
Create data tools alongside Data Analysts and Data Scientists in a collaborative fashion to support business outcomes
Work alongside the rest of the Data team to enable best use of our Looker instance, ideally through writing and optimising LookML.
Requirements
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL), as well as working familiarity with a variety of databases (PostgreSQL and MySQL preferably).
Database administration and data performance management, including the details of indices, normalisation, query optimisation and resultant execution plans.
Building and optimising ‘big data’ data pipelines, architectures and data sets.
Root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores
The successful candidate will have 3+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Informatics, Information Systems or another quantitative field.
Must-haves:
Cloud Based ETL Tools: Stitch, FiveTran, Matillion.
Relational SQL and NoSQL databases: MySQL, Postgres, Redshift and DynamoDB
Data pipeline and workflow management tools: DBT, Dataform, AWS Glue, AWS Data pipeline, Singer.io, Luigi, Airflow, etc.
Data visualisation tools: Looker (including LookML experience)
AWS cloud services: EC2, EMR, RDS, Redshift
Object-oriented/object function scripting languages: Go, PHP, Python, Scala, etc.
Nice to haves:
Kafka, Hadoop and Spark
R and Jupyter Notebooks
Benefits
Company laptop (Mac or PC - you choose!)
Choice of wellbeing benefits which include Private Medical Insurance and/or a gym membership
20 days holiday plus bank holidays
Friday drinks
Team lunches
Team socials every month
Free TouchNote credits to use our product!
Reporting to Data Lead

Salary competitive and dependent on experience

This is a full-time position (Monday - Friday: 11AM - 8PM)",Data Engineer,Hyderabad,5.0,$10 to $25 million (USD),"INR  7,86,296",Information Technology,1 to 50 Employees,Company - Private
"Amazon.com, Inc.
4.3",-1,1994,-1,Internet,"The TRON team is an initiative which partners with the Amazon Robotics team to remotely handle exceptions in the Amazon Robotic Fulfillment Centers in North America. The TRON technology enables the human supervisory control of automated tasks. In the TRON system, a remote associate provides supervisory control when automated tasks fail for any reason. This allows us to avoid falling into the 90/10 trap where a task can be 90% automated but requires 90% of the time for the last 10% of functionality. Our strategy is to employ a human-as-sensor model and allow humans to perform just those portions of a task that require higher order cognitive ability.
TRON team is looking for a Data Engineer. As a Data Engineer on TRON team your responsibilities will include

Responsibilities include:
· Apply broad knowledge of technology options, technology platforms, design techniques and approaches across the Data Engineering ecosystem to design systems that meet business needs.
· Play a leading role in building systems and datasets using software engineering best practices, data management fundamentals, data storage principles, recent advances in distributed systems, and operational excellence best practices.
· Play a leading role in architecture design and implementation of next generation BI solutions using Big Data and AWS Services.
· Analyze source systems, define underlying data sources and transformation requirements, design suitable data models and document the design/specifications.
· Demonstrate passion for quality and productivity by use of efficient development techniques, standards and guidelines.
· Effectively communicate with various teams and stakeholders, escalate technical and managerial issues at the right time and resolve conflicts.
· Peer review work. Actively mentor more junior members of the team, improving their skills, their knowledge of our systems and their ability to get things done.
Manage AWS resources including EC2, RDS, Redshift, Kinesis, EMR, Lambda etc.

Basic Qualifications

· 1+ years of experience as a Data Engineer or in a similar role
· Experience with data warehousing, and building ETL pipelines
· Experience in SQL
· Bachelor's degree or higher in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering).
· 3+ years of relevant experience in one of the following areas: Data engineering, database engineering, business intelligence or business analytics.
· 3+ years of hands-on experience in writing complex, highly-optimized SQL queries across large data sets.
· 2+ years of experience in scripting languages like Python etc.
· Demonstrated strength in data modeling, ETL development, and Data warehousing.
· Experience with AWS services including S3, Redshift, EMR and RDS.
· Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.)
· Experience in working and delivering end-to-end projects independently.
· Knowledge of distributed systems as it pertains to data storage and computing


Preferred Qualifications

· Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
· Experience providing technical leadership and mentoring other engineers for best practices on data engineering
· Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations

Amazon is an equal opportunities employer. We believe passionately that employing a diverse workforce is central to our success. We make recruiting decisions based on your experience and skills. We value your passion to discover, invent, simplify and build. We welcome applications from all members of society irrespective of age, sex, disability, sexual orientation, race, religion or belief

·",Data Engineer I,Hyderabad,4.3,$10+ billion (USD),"INR  14,39,361",Information Technology,10000+ Employees,Company - Public
"PayPal, Inc.
3.7",-1,1998,-1,Internet,"Fueled by a fundamental belief that having access to financial services creates opportunity, PayPal (NASDAQ: PYPL) is committed to democratizing financial services and empowering people and businesses to join and thrive in the global economy. Our open digital payments platform gives PayPals 305 million active account holders the confidence to connect and transact in new and powerful ways, whether they are online, on a mobile device, in an app, or in person. Through a combination of technological innovation and strategic partnerships, PayPal creates better ways to manage and move money, and offers choice and flexibility when sending payments, paying or getting paid. Available in more than 200 markets around the world, the PayPal platform, including Braintree, Venmo and Xoom enables consumers and merchants to receive money in more than 100 currencies, withdraw funds in 56 currencies and hold balances in their PayPal accounts in 25 currencies.

With over 300m registered active users and strong double digit growth, PayPal is working hard to provide expanded services to our customers. We have a unique opportunity for a talented engineer in PayPal Data Governance platform team. We are looking for a smart, execution focused technical engineer that will be responsible for working on key deliverables on PayPal Data Governance Platform, working closely with many internal technology partners to deliver key platform capabilities that will allow our internal business partners to match their expectation from the Data Governance platform.

Responsibilities:
Build scalable systems, lead technical discussions, participate in code reviews, guide the team in engineering best practices. Must be able to write quality code and build secure, highly available systems. 75% of the job requires production quality coding.
Provide technical insights and contribute to the definition, development, integration, test, documentation, and support across multiple platforms
Establish a consistent, project management framework and development processes to deliver high quality software, in rapid iterations, for business partners in multiple geographies
Must have demonstrably strong interpersonal and communication skills (both written and verbal), to include speaking clearly and persuasively in positive or negative situations
Eat, sleep, and breathe services. Experienced in balancing production platform stability, feature delivery, and retirement of technical debt across a broad landscape of technologies
Qualifications:
Undergraduate degree in Computer Engineering or equivalent from a leading university and preferably with a Masters
7+ years of post-college working experience as a developer and architect in Engineering, or Data analytics organization
Excellent knowledge of Java with hands on experience
Understanding and background of distributed data processing
Knowledge of batch and real-time processing
Must have experience working with cloud environment
Good understanding of Hadoop ecosystem i.e. map reduce, Tez, llap and hdfs
Knowledge on one of the streaming platforms Kafka, rabbitMQ with at least once, at most once and exactly once patterns
Must have a good understanding of spark, rdd, data frame Api
Must have experience with performance improvement for big data job
Experience with Altus, Raptur is a plus
Knowledge of Scala will be a plus
Knowledge of scripting language e.g. python is a plus
Strong conceptual and creative problem-solving skills; ability to work with considerable ambiguity; ability to learn new and complex concepts quickly. Relentlessly resourceful and scrappy
A great communicator, strong project management skills, and superb attention to details",Sr Big Data Engineer,Chennai,3.7,$10+ billion (USD),"INR  12,55,505",Information Technology,10000+ Employees,Company - Public
"NGDATA, Inc.
3.0",-1,2012,-1,Enterprise Software & Network Solutions,"“I am a trusted advisor for our customers, partners and colleagues. I am eager to learn and have the freedom to grow.”

NGDATA’s creed and our way of maintaining this creed is greatly impacted by the way we deliver projects. As a Big Data Engineer at NGDATA, you will get the chance to write, implement and configure innovative software, using the newest development practices. A passion for all aspects of software development, as well as customer-facing, are therefore of high importance.

In this role, you will join a highly motivated Professional Services team consisting out of 10 people and will be able to have a significant impact on our business by implementing data-centric enterprise software.
Opportunities
Get a taste of the newest technologies such as the Hadoop stack, Microsoft Azure, Cloud stack, AWS, etc.;
Be a part of a strong and quickly evolving technical space;
Get the opportunity to work at a variety of customers;
Be amazed by the amount of responsibility you can take up as a new employee under the guidance of your more senior co-workers.
Responsibilities
Implement our NGDATA solutions on site or cloud based at customers with a wide variety of infra and architectures;
Help clients define solutions and bring technical guidance both related to our solution as more broad related issues;
Deliver clear and exhaustive design documents;
Develop exemplary and thoroughly tested code for the most challenging aspects of our product or product sub-systems;
Perform quality assurance on all implementations from inception through integration, staging and production environments;
Collaborate effectively with customers as well as other technology teams and architects to solve complex challenges spanning their respective areas;
Exhibit creativity and resourcefulness at problem-solving while collaborating and working effectively with customers, designers, engineers of different technical backgrounds, architects and product managers.
Personal Skills
Consultative approach and behavior;
Open minded and solution oriented;
Persistent, accurate, creative;
A logical approach to the solution of problems and good conceptual ability and skills in analysis;
Ability to integrate research and best practices into problem-solving and continuous improvement;
A true team player who isn’t shy of working independently on projects;
Discipline to document and record results;
Enjoying constantly expanding your knowledge base;
Willing to work at the customer location and travel regularly.
Technical background
You should have 5+ years of experience in large-scale software development and consultancy. You have a strong cross-functional technical background and a willingness and capacity to expand leadership and technical skills;
Fluent in English;
knowledge in developing and debugging in Java/J2EE;
Maven, git, IntelliJ/Eclipse, unit testing;
Proficient Linux user with basic Linux administration skills;
Experience with agile/scrum methodologies to iterate quickly on product changes, develop user stories and work through backlogs;
Exercise independent judgment in methods, techniques and evaluation criteria for obtaining results;
Experience in working with Hadoop/MapReduce, HBase, Hive, Flume, SOLR is considered a plus;
Familiarity with No-SQL and ETL tools is considered a plus;
Knowledgeable of the RESTful concept is considered a plus;
Experience with Python is considered a plus;
Knowledge of data modeling and programming is considered a plus;
Experience with performance tuning and profiling is considered a plus.",Big Data Engineer,Chennai,3.0,Less than $1 million (USD),INR 513K - INR 763K,Information Technology,51 to 200 Employees,Company - Private
